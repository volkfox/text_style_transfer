{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_Discriminator_DistilBert_masked.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sI6DKVblH_CD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4-fSro-v_v-",
        "colab_type": "code",
        "outputId": "782ae14a-a8c7-4efb-89e9-355f3cd6032a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "# Install the required modules\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install tqdm\n",
        "!pip install sklearn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# !nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/1a/364556102943cacde1ee00fdcae3b1615b39e52649eddbf54953e5b144c9/transformers-2.2.1-py3-none-any.whl (364kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 55.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 52.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.32)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 50.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.32 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.32)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=2aa91b172c5aa1df5740e62954872e6532b496309b623c60489f90b51cec38ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, regex, transformers\n",
            "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.2.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.4)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.17.4)\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuAmirw-wSEV",
        "colab_type": "code",
        "outputId": "75c7bcf8-43a5-4b66-974b-0036908579d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import *"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9OtFNVx0vB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Embeddings can be derived from the last 1 or 4 layers, to reduce the computational cost, we used only the last layer.\n",
        "\n",
        "class Embeddings:\n",
        "    LAST_LAYER = 1\n",
        "    LAST_4_LAYERS = 2\n",
        "    def __init__(self):\n",
        "        self._tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self._bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "        self._bert_model.eval()\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :return: tokenized sentence based on word piece model ['List']\n",
        "        \"\"\"\n",
        "        marked_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "        tokenized_text = self._tokenizer.tokenize(marked_sentence)\n",
        "        return tokenized_text\n",
        "\n",
        "    def get_bert_embeddings(self, sentence):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :return: BERT pre-trained hidden states (list of torch tensors) ['List']\n",
        "        \"\"\"\n",
        "        # Predict hidden states features for each layer\n",
        "\n",
        "        tokenized_text = self.tokenize(sentence)\n",
        "        indexed_tokens = self._tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "        segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "        # Convert inputs to PyTorch tensors\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "        segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoded_layers = self._bert_model(tokens_tensor, segments_tensors)\n",
        "\n",
        "        return encoded_layers[-1][0:12]\n",
        "\n",
        "    def sentence2vec(self, sentence, layers):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :param layers: parameter to decide how word embeddings are obtained ['str]\n",
        "            1. 'last' : last hidden state used to obtain word embeddings for sentence tokens\n",
        "            2. 'last_4' : last 4 hidden states used to obtain word embeddings for sentence tokens\n",
        "\n",
        "        :return: sentence vector [List]\n",
        "        \"\"\"\n",
        "        encoded_layers = self.get_bert_embeddings(sentence)\n",
        "        \n",
        "        if layers == 1:\n",
        "            # using the last layer embeddings\n",
        "            token_embeddings = encoded_layers[-1]\n",
        "            # summing the last layer vectors for each token\n",
        "            sentence_embedding = torch.mean(token_embeddings, 1)\n",
        "            return sentence_embedding.view(-1).tolist()\n",
        "\n",
        "        elif layers == 2:\n",
        "            token_embeddings = []\n",
        "            tokenized_text = self.tokenize(sentence)\n",
        "\n",
        "            batch_i = 0\n",
        "            # For each token in the sentence...\n",
        "            for token_i in range(len(tokenized_text)):\n",
        "\n",
        "                # Holds 12 layers of hidden states for each token\n",
        "                hidden_layers = []\n",
        "\n",
        "                # For each of the 12 layers...\n",
        "                for layer_i in range(len(encoded_layers)):\n",
        "                    # Lookup the vector for `token_i` in `layer_i`\n",
        "                    vec = encoded_layers[layer_i][batch_i][token_i]\n",
        "\n",
        "                    hidden_layers.append(list(vec.numpy()))\n",
        "\n",
        "                token_embeddings.append(hidden_layers)\n",
        "\n",
        "            # using the last 4 layer embeddings\n",
        "            token_vecs_sum = []\n",
        "\n",
        "            # For each token in the sentence...\n",
        "            for token in token_embeddings:\n",
        "                # Sum the vectors from the last four layers.\n",
        "                sum_vec = np.sum(token[-4:], axis=0)\n",
        "\n",
        "                # Use `sum_vec` to represent `token`.\n",
        "                token_vecs_sum.append(list(sum_vec))\n",
        "\n",
        "            # summing the last layer vectors for each token\n",
        "            sentence_embedding = np.mean(token_vecs_sum, axis=0)\n",
        "            return sentence_embedding.ravel().tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBmksfP_04cH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset: 3000 chunks * 3 authors, without masking\n",
        "\n",
        "# url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/Datasets/raw_text_3000.csv'\n",
        "# df = pd.read_csv(url)\n",
        "\n",
        "# X = df.text.astype('str')\n",
        "# y = df.author.astype('category')\n",
        "\n",
        "# # lbl_enc = preprocessing.LabelEncoder()\n",
        "# # y = lbl_enc.fit_transform(y.values)\n",
        "\n",
        "# y = np.asarray(y)\n",
        "# onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
        "# encoded = y.reshape(len(y), 1)\n",
        "# y = onehot_encoder.fit_transform(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtkqPiy6v2VR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset: 3000 chunks * 3 authors, with masking\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/Datasets/masked_text_3000.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "X = df.text.astype('str')\n",
        "y = df.author.astype('category')\n",
        "\n",
        "# lbl_enc = preprocessing.LabelEncoder()\n",
        "# y = lbl_enc.fit_transform(y.values)\n",
        "\n",
        "y = np.asarray(y)\n",
        "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
        "encoded = y.reshape(len(y), 1)\n",
        "y = onehot_encoder.fit_transform(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD1A_uzX-mUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = Embeddings()\n",
        "\n",
        "# X_text = []\n",
        "# for sentence in tqdm(X):\n",
        "#     X_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auQfIbHooxmk",
        "colab_type": "code",
        "outputId": "053df7e1-bc17-40aa-d3b5-a25c897c7ba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# For Test Time\n",
        "\n",
        "# url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/masked_results.csv'\n",
        "# dft = pd.read_csv(url).set_index('Unnamed: 0')\n",
        "\n",
        "# X_old = dft.old.astype('str')\n",
        "# X_new = dft.new.astype('str')\n",
        "\n",
        "# model = Embeddings()\n",
        "\n",
        "# X_old_text, X_new_text = [], []\n",
        "# for sentence in tqdm(X_old):\n",
        "#     X_old_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))\n",
        "# for sentence in tqdm(X_new):\n",
        "#     X_new_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 925756.54B/s]\n",
            "100%|██████████| 313/313 [00:00<00:00, 74216.58B/s]\n",
            "100%|██████████| 440473133/440473133 [00:16<00:00, 26964703.15B/s]\n",
            "100%|██████████| 55/55 [00:31<00:00,  1.69it/s]\n",
            "100%|██████████| 55/55 [00:34<00:00,  1.57it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7_YrIamp5Dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_old = pd.DataFrame(X_old_text)\n",
        "# X_old.to_csv('./gdrive/My Drive/DL/Style/X_old_Embedding.csv')\n",
        "\n",
        "# X_new = pd.DataFrame(X_new_text)\n",
        "# X_new.to_csv('./gdrive/My Drive/DL/Style/X_new_Embedding.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nplxzc29EEzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_df = pd.DataFrame(X_text)\n",
        "# X_df.to_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000_2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16TunKA3KVd1",
        "colab_type": "code",
        "outputId": "ad8c70dc-3039-4242-d696-c53bd38f694f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# For old Rand examples\n",
        "\n",
        "# url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/rand_examples.csv'\n",
        "# df = pd.read_csv(url).set_index('Unnamed: 0')\n",
        "\n",
        "\n",
        "# X1 = df['Rand-donor-text'].astype('str')\n",
        "# X2 = df['Rand_117M_10000_Nabokov-All-3'].astype('str')\n",
        "# X3 = df['Rand-output-ngram'].astype('str')\n",
        "\n",
        "# model = Embeddings()\n",
        "\n",
        "# X1_text, X2_text, X3_text = [], [], []\n",
        "# for sentence in tqdm(X1):\n",
        "#     X1_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))\n",
        "# for sentence in tqdm(X2):\n",
        "#     X2_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))\n",
        "# for sentence in tqdm(X2):\n",
        "#     X3_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1184340.76B/s]\n",
            "100%|██████████| 313/313 [00:00<00:00, 48256.47B/s]\n",
            "100%|██████████| 440473133/440473133 [00:12<00:00, 36565607.31B/s]\n",
            "100%|██████████| 7/7 [00:02<00:00,  3.65it/s]\n",
            "100%|██████████| 7/7 [00:02<00:00,  3.43it/s]\n",
            "100%|██████████| 7/7 [00:02<00:00,  3.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZQlnt96NhI9",
        "colab_type": "text"
      },
      "source": [
        "### Building A Style Classifier on Top of DistilBert Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY3FCnoh-zc4",
        "colab_type": "code",
        "outputId": "5670965d-696e-4bf8-b01c-6c6324417c80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# X_df = pd.DataFrame(X_text)\n",
        "# X_df.to_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000.csv')\n",
        "\n",
        "X_df = pd.read_csv('./gdrive/My Drive/DL/Style/DistilBert_Embedding_3000_2.csv').set_index('Unnamed: 0')\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_df, y, stratify=y, random_state=1, test_size=0.2, shuffle=True)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=1, shuffle=True)\n",
        "\n",
        "print(X_train.shape, X_val.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7200, 768) (900, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gws6Gds8iSsq",
        "colab_type": "code",
        "outputId": "3aee1c36-c231-4974-acf4-039c6cc90ef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Feed-Forward Neural Nets\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): the size of the input vectors\n",
        "            hidden_dim (int): the output size of the first Linear layer\n",
        "            output_dim (int): the output size of the second Linear layer\n",
        "        \"\"\"\n",
        "        super(FFNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.bn3 = nn.BatchNorm1d(output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"The forward pass of the FFNN\n",
        "        \n",
        "        Args:\n",
        "            x (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, input_dim)\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        c = self.fc1(x)\n",
        "        x = self.bn1(c)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        c = torch.cat((x, c), 1)\n",
        "        x = self.fc2(c)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        c = torch.cat((x, c), 1)\n",
        "        x = self.fc3(c)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        output = F.dropout(x, p=0.5)\n",
        "     \n",
        "        return output\n",
        "\n",
        "batch_size = 32 # number of samples input at once\n",
        "input_dim = 768\n",
        "hidden_dim = 128\n",
        "output_dim = 3\n",
        "\n",
        "# Initialize model\n",
        "model = FFNN(input_dim, hidden_dim, output_dim)\n",
        "print(model)\n",
        "\n",
        "X = torch.tensor(np.array(X_train))\n",
        "# y_output = model(X)\n",
        "# describe(y_output)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNN(\n",
            "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
            "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
            "  (bn3): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI6DKVblH_CD",
        "colab_type": "text"
      },
      "source": [
        "### Baselines\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uDpIahUMo8X",
        "colab_type": "code",
        "outputId": "64095b39-c23e-4b12-e604-c4147d51a844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# For Baselines\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/Datasets/masked_text_3000.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "X = df.text.astype('str')\n",
        "y = df.author.astype('category')\n",
        "\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "y = lbl_enc.fit_transform(y.values)\n",
        "\n",
        "xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify=y, random_state=1, test_size=0.2, shuffle=True)\n",
        "xvalid, xtest, yvalid, ytest = train_test_split(xvalid, yvalid, test_size=0.5, random_state=1, shuffle=True)\n",
        "\n",
        "print(xtrain.shape, xvalid.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7200,) (900,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03oQGXQJIIhj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4fb27158-b71b-4480-ddea-e8586b11d7f6"
      },
      "source": [
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.svm import SVC\n",
        "from keras.models import Sequential\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "from tqdm import tqdm\n",
        "import xgboost as xgb\n",
        "import sys\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk import word_tokenize\n",
        "stop_words = stopwords.words('english')\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow_hub as hub\n",
        "%matplotlib inline\n",
        "plt.xkcd()\n",
        "\n",
        "# Defining Multi-Class Log Loss\n",
        "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
        "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
        "    :param actual: Array containing the actual target classes\n",
        "    :param predicted: Matrix with class predictions, one probability per class\n",
        "    \"\"\"\n",
        "    # Convert 'actual' to a binary array if it's not already:\n",
        "    if len(actual.shape) == 1:\n",
        "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
        "        for i, val in enumerate(actual):\n",
        "            actual2[i, val] = 1\n",
        "        actual = actual2\n",
        "\n",
        "    clip = np.clip(predicted, eps, 1 - eps)\n",
        "    rows = actual.shape[0]\n",
        "    vsota = np.sum(actual * np.log(clip))\n",
        "    return -1.0 / rows * vsota"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alCe8WVxNjK7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "d1fb67fe-9006-4a39-9645-d4d78cc4a15b"
      },
      "source": [
        "#TF-IDF features\n",
        "tfv = TfidfVectorizer(min_df=3,\n",
        "                      max_features=None,\n",
        "                      strip_accents='unicode',\n",
        "                      analyzer='word',\n",
        "                      token_pattern=r'\\w{1,}',\n",
        "                      ngram_range=(1,3),\n",
        "                      use_idf=1,\n",
        "                      smooth_idf=1,\n",
        "                      sublinear_tf=1,\n",
        "                      stop_words='english')\n",
        "\n",
        "tfv.fit(list(xtrain) + list(xvalid))\n",
        "xtrain_tfv = tfv.transform(xtrain)\n",
        "xvalid_tfv = tfv.transform(xvalid)\n",
        "\n",
        "print(\"The size of the learnt vocabulary is \", len(tfv.vocabulary_))\n",
        "\n",
        "def display_scores(vectorizer, tfidf_result):\n",
        "    scores = zip(vectorizer.get_feature_names(), np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
        "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "    count = 0\n",
        "    for item in sorted_scores: \n",
        "        print(item[0], item[1])\n",
        "        count += 1\n",
        "        if count>=20:\n",
        "            break\n",
        "\n",
        "#See top 10 TF-IDF scores\n",
        "print('Top 20 TF-IDF scores are --- \\n')\n",
        "display_scores(tfv,xvalid_tfv)\n",
        "\n",
        "\n",
        "\n",
        "#Logistic Regression on TF-IDF\n",
        "clf_tfidf = LogisticRegression(C=1.0)\n",
        "clf_tfidf.fit(xtrain_tfv, ytrain)\n",
        "predictions = clf_tfidf.predict_proba(xvalid_tfv)\n",
        "pred = clf_tfidf.predict(xvalid_tfv)\n",
        "print(\"logloss for TF-IDF + LR : \" + str(multiclass_logloss(yvalid, predictions)))\n",
        "print(classification_report(yvalid, pred))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of the learnt vocabulary is  24138\n",
            "Top 20 TF-IDF scores are --- \n",
            "\n",
            "mask 42.3373247275727\n",
            "mask mask 19.49016758000067\n",
            "s 19.264278950832434\n",
            "said 18.40952044299222\n",
            "mask s 11.89549140304362\n",
            "said mask 10.86659421311315\n",
            "time 10.83122311068303\n",
            "d 9.641361705463183\n",
            "did 9.465040800253758\n",
            "like 9.374128794686039\n",
            "good 9.262415173964058\n",
            "know 9.161314605106275\n",
            "little 9.152136391660441\n",
            "d mask 8.698420964845504\n",
            "man 8.46385578455\n",
            "mask mask mask 8.412836404979416\n",
            "mr 8.283823936742474\n",
            "shall 8.257588996039829\n",
            "king 8.07012618864858\n",
            "great 7.981449294301742\n",
            "logloss for TF-IDF + LR : 0.3384619160096547\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97       301\n",
            "           1       0.98      0.95      0.97       306\n",
            "           2       0.93      0.98      0.95       293\n",
            "\n",
            "    accuracy                           0.96       900\n",
            "   macro avg       0.96      0.96      0.96       900\n",
            "weighted avg       0.96      0.96      0.96       900\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySv4hX9IOzpB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "3be96d3a-349e-43eb-b3fb-b62c67f2474b"
      },
      "source": [
        "ctv = CountVectorizer(analyzer='word',\n",
        "                      token_pattern=r'\\w{1,}',\n",
        "                      ngram_range=(1,3),\n",
        "                      stop_words='english')\n",
        "\n",
        "ctv.fit(list(xtrain) + list(xvalid))\n",
        "xtrain_ctv = ctv.transform(xtrain)\n",
        "xvalid_ctv = ctv.transform(xvalid)\n",
        "\n",
        "clf_ctv = LogisticRegression(C=1.0)\n",
        "clf_ctv.fit(xtrain_ctv, ytrain)\n",
        "predictions = clf_ctv.predict_proba(xvalid_ctv)\n",
        "predictions_rounded = clf_ctv.predict(xvalid_ctv)\n",
        "print(\"logloss for CVectorizer + LR : \", multiclass_logloss(yvalid, predictions))\n",
        "print(classification_report(yvalid, predictions_rounded))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logloss for CVectorizer + LR :  0.13450918346659163\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97       301\n",
            "           1       0.98      0.95      0.96       306\n",
            "           2       0.94      0.97      0.95       293\n",
            "\n",
            "    accuracy                           0.96       900\n",
            "   macro avg       0.96      0.96      0.96       900\n",
            "weighted avg       0.96      0.96      0.96       900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHoVJDM7ICKk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "afb66a7f-ffa5-474c-c591-bec8440e96d0"
      },
      "source": [
        "# TF-IDF + NB\n",
        "#Multinomial Naive Bayes on TF-IDF\n",
        "clf = MultinomialNB()\n",
        "clf.fit(xtrain_tfv, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_tfv)\n",
        "predictions_rounded = clf.predict(xvalid_tfv)\n",
        "print(\"logloss for Multinomial Naive Bayes: \", multiclass_logloss(yvalid, predictions))\n",
        "print(classification_report(yvalid, predictions_rounded))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logloss for Multinomial Naive Bayes:  0.23024957811771907\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       301\n",
            "           1       0.98      0.98      0.98       306\n",
            "           2       0.98      0.97      0.97       293\n",
            "\n",
            "    accuracy                           0.98       900\n",
            "   macro avg       0.98      0.98      0.98       900\n",
            "weighted avg       0.98      0.98      0.98       900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TCgHhrhPUBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8d5245be-6732-42a3-e5af-7333e3cb9886"
      },
      "source": [
        "#Multinomial Naive Bayes on Count Vectors\n",
        "clf.fit(xtrain_ctv, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_ctv)\n",
        "predictions_rounded = clf.predict(xvalid_ctv)\n",
        "print(\"logloss for Multinomial Naive Bayes with count vectorizer : \", multiclass_logloss(yvalid, predictions))\n",
        "print(classification_report(yvalid, predictions_rounded))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logloss for Multinomial Naive Bayes with count vectorizer :  0.18638961017304032\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99       301\n",
            "           1       0.95      0.99      0.97       306\n",
            "           2       0.99      0.94      0.97       293\n",
            "\n",
            "    accuracy                           0.97       900\n",
            "   macro avg       0.98      0.97      0.97       900\n",
            "weighted avg       0.97      0.97      0.97       900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCYSay6-PdKN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "714841d8-a409-4c9d-f77f-fe6be926f62a"
      },
      "source": [
        "# SVD on TF-IDF --> SVM\n",
        "\n",
        "svd = decomposition.TruncatedSVD(n_components=100)\n",
        "svd.fit(xtrain_tfv)\n",
        "xtrain_svd = svd.transform(xtrain_tfv)\n",
        "xvalid_svd = svd.transform(xvalid_tfv)\n",
        "\n",
        "scl = preprocessing.StandardScaler()\n",
        "scl.fit(xtrain_svd)\n",
        "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
        "xvalid_svd_scl = scl.transform(xvalid_svd)\n",
        "\n",
        "clf = SVC(C=1.0, probability=True)\n",
        "clf.fit(xtrain_svd_scl, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_svd_scl)\n",
        "predictions_rounded = clf.predict(xvalid_svd_scl)\n",
        "print(\" SVM logloss : \", multiclass_logloss(yvalid, predictions))\n",
        "print(classification_report(yvalid, predictions_rounded))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVM logloss :  0.16455732235237291\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.95       301\n",
            "           1       0.98      0.92      0.95       306\n",
            "           2       0.90      0.99      0.94       293\n",
            "\n",
            "    accuracy                           0.95       900\n",
            "   macro avg       0.95      0.95      0.95       900\n",
            "weighted avg       0.95      0.95      0.95       900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88k8iwQ6V5FC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# XGBoost on SVD features\n",
        "clf = xgb.XGBClassifier()\n",
        "clf.fit(xtrain_svd, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_svd)\n",
        "predictions_rounded = clf.predict(xvalid_svd)\n",
        "\n",
        "print (\"XGBOOST logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
        "print(classification_report(yvalid, predictions_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlgV4SWjWS3t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "3e191da6-c1c1-477d-a00c-869a3c90c81b"
      },
      "source": [
        "# !wget http://www-nlp.stanford.edu/data/glove.840B.300d.zip"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-07 21:52:41--  http://www-nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving www-nlp.stanford.edu (www-nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to www-nlp.stanford.edu (www-nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2019-12-07 21:52:42--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2019-12-07 21:52:42--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  1.94MB/s    in 16m 58s \n",
            "\n",
            "2019-12-07 22:09:40 (2.04 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opAaqtHuWdIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "753cf871-44cf-41cd-e95d-331aecfeb75e"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open('/content/gdrive/My Drive/DL/NLP/GloVe/glove.6B.300d.txt')\n",
        "for line in tqdm(f):\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs        \n",
        "    except ValueError:\n",
        "        pass\n",
        "    \n",
        "f.close()\n",
        "print(\"Found \",len(embeddings_index),\"vectors\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000it [00:33, 11767.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found  400000 vectors\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvmsZcBEWiIA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "03d87fc8-6d3f-4d1c-d87a-887e2267e495"
      },
      "source": [
        "# generate weighted sentence embeddings\n",
        "\n",
        "def sent2vec(s):\n",
        "    words = str(s).lower()\n",
        "    words = word_tokenize(words)\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    words = [w for w in words if w.isalpha()]\n",
        "    M = []\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(embeddings_index[w])\n",
        "        except:\n",
        "            continue\n",
        "    M = np.array(M)\n",
        "    v = M.sum(axis=0)\n",
        "    if type(v) != np.ndarray:\n",
        "        return np.zeros(300)\n",
        "    return v / np.sqrt((v ** 2).sum())\n",
        "\n",
        "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
        "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7200/7200 [00:08<00:00, 861.22it/s]\n",
            "100%|██████████| 900/900 [00:01<00:00, 873.14it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Crwv4UzQWl93",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "297346c5-6883-401e-9c9b-5fdf6ddd8d90"
      },
      "source": [
        "xtrain_glove = np.array(xtrain_glove)\n",
        "xvalid_glove = np.array(xvalid_glove)\n",
        "\n",
        "clf = LogisticRegression(C=1.0)\n",
        "clf.fit(xtrain_glove, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_glove)\n",
        "predictions_rounded = clf.predict(xvalid_glove)\n",
        "print(\"logloss : \", multiclass_logloss(yvalid, predictions))\n",
        "print(classification_report(yvalid, predictions_rounded))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logloss :  0.39487923541037245\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.94      0.92       301\n",
            "           1       0.93      0.87      0.90       306\n",
            "           2       0.90      0.91      0.91       293\n",
            "\n",
            "    accuracy                           0.91       900\n",
            "   macro avg       0.91      0.91      0.91       900\n",
            "weighted avg       0.91      0.91      0.91       900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDkSeVMZWpbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scale the data before any neural net:\n",
        "scl = preprocessing.StandardScaler()\n",
        "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
        "xvalid_glove_scl = scl.transform(xvalid_glove)\n",
        "\n",
        "\n",
        "# we need to binarize the labels for the neural net\n",
        "ytrain_enc = np_utils.to_categorical(ytrain)\n",
        "yvalid_enc = np_utils.to_categorical(yvalid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku2YuApUWs34",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "507eac17-4a5a-4ba2-a915-2c56488d8f2f"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(300, input_dim=300, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(300, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(3))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam')\n",
        "\n",
        "model.fit(xtrain_glove_scl,\n",
        "          y=ytrain_enc,\n",
        "          batch_size=64, \n",
        "          epochs=5,\n",
        "          verbose=1, \n",
        "          validation_data=(xvalid_glove_scl, yvalid_enc))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "7200/7200 [==============================] - 9s 1ms/step - loss: 0.4825 - val_loss: 0.2730\n",
            "Epoch 2/5\n",
            "7200/7200 [==============================] - 1s 115us/step - loss: 0.2642 - val_loss: 0.2489\n",
            "Epoch 3/5\n",
            "7200/7200 [==============================] - 1s 112us/step - loss: 0.1991 - val_loss: 0.2546\n",
            "Epoch 4/5\n",
            "7200/7200 [==============================] - 1s 104us/step - loss: 0.1679 - val_loss: 0.2335\n",
            "Epoch 5/5\n",
            "7200/7200 [==============================] - 1s 118us/step - loss: 0.1395 - val_loss: 0.2456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e1cf424e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQfp3KKgWvm8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b419c40b-4459-461a-8254-d7e324f79c54"
      },
      "source": [
        "# LSTM\n",
        "token = text.Tokenizer(num_words=None)\n",
        "max_len = 70\n",
        "\n",
        "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
        "xtrain_seq = token.texts_to_sequences(xtrain)\n",
        "xvalid_seq = token.texts_to_sequences(xvalid)\n",
        "\n",
        "# zero pad the sequences\n",
        "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
        "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
        "word_index = token.word_index\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "for word, i in tqdm(word_index.items()):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# A simple LSTM with glove embeddings and two dense layers\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     weights=[embedding_matrix],\n",
        "                     input_length=max_len,\n",
        "                     trainable=False))\n",
        "model.add(SpatialDropout1D(0.3))\n",
        "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
        "\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(3))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "model.fit(xtrain_pad,\n",
        "          y=ytrain_enc,\n",
        "          batch_size=512,\n",
        "          epochs=100, \n",
        "          verbose=1,\n",
        "          validation_data=(xvalid_pad, yvalid_enc),\n",
        "          callbacks=[earlystop])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31382/31382 [00:00<00:00, 522007.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/100\n",
            "7200/7200 [==============================] - 3s 382us/step - loss: 1.1068 - val_loss: 1.0563\n",
            "Epoch 2/100\n",
            "7200/7200 [==============================] - 2s 262us/step - loss: 1.0561 - val_loss: 0.9654\n",
            "Epoch 3/100\n",
            "7200/7200 [==============================] - 2s 260us/step - loss: 0.9717 - val_loss: 0.6923\n",
            "Epoch 4/100\n",
            "7200/7200 [==============================] - 2s 278us/step - loss: 0.8319 - val_loss: 0.5786\n",
            "Epoch 5/100\n",
            "7200/7200 [==============================] - 2s 232us/step - loss: 0.7711 - val_loss: 0.4971\n",
            "Epoch 6/100\n",
            "7200/7200 [==============================] - 2s 250us/step - loss: 0.7074 - val_loss: 0.4626\n",
            "Epoch 7/100\n",
            "7200/7200 [==============================] - 2s 274us/step - loss: 0.6805 - val_loss: 0.4425\n",
            "Epoch 8/100\n",
            "7200/7200 [==============================] - 2s 271us/step - loss: 0.6346 - val_loss: 0.4172\n",
            "Epoch 9/100\n",
            "7200/7200 [==============================] - 2s 252us/step - loss: 0.5946 - val_loss: 0.3981\n",
            "Epoch 10/100\n",
            "7200/7200 [==============================] - 2s 241us/step - loss: 0.5709 - val_loss: 0.3899\n",
            "Epoch 11/100\n",
            "7200/7200 [==============================] - 2s 235us/step - loss: 0.5328 - val_loss: 0.3478\n",
            "Epoch 12/100\n",
            "7200/7200 [==============================] - 2s 239us/step - loss: 0.5056 - val_loss: 0.3146\n",
            "Epoch 13/100\n",
            "7200/7200 [==============================] - 2s 239us/step - loss: 0.4807 - val_loss: 0.3046\n",
            "Epoch 14/100\n",
            "7200/7200 [==============================] - 2s 255us/step - loss: 0.4791 - val_loss: 0.2895\n",
            "Epoch 15/100\n",
            "7200/7200 [==============================] - 2s 232us/step - loss: 0.4403 - val_loss: 0.2560\n",
            "Epoch 16/100\n",
            "7200/7200 [==============================] - 2s 245us/step - loss: 0.4251 - val_loss: 0.2448\n",
            "Epoch 17/100\n",
            "7200/7200 [==============================] - 2s 298us/step - loss: 0.3958 - val_loss: 0.2441\n",
            "Epoch 18/100\n",
            "7200/7200 [==============================] - 2s 277us/step - loss: 0.3798 - val_loss: 0.2228\n",
            "Epoch 19/100\n",
            "7200/7200 [==============================] - 2s 256us/step - loss: 0.3824 - val_loss: 0.2161\n",
            "Epoch 20/100\n",
            "7200/7200 [==============================] - 2s 247us/step - loss: 0.3623 - val_loss: 0.2141\n",
            "Epoch 21/100\n",
            "7200/7200 [==============================] - 2s 253us/step - loss: 0.3569 - val_loss: 0.2075\n",
            "Epoch 22/100\n",
            "7200/7200 [==============================] - 2s 234us/step - loss: 0.3372 - val_loss: 0.2110\n",
            "Epoch 23/100\n",
            "7200/7200 [==============================] - 2s 224us/step - loss: 0.3218 - val_loss: 0.2027\n",
            "Epoch 24/100\n",
            "7200/7200 [==============================] - 2s 252us/step - loss: 0.3062 - val_loss: 0.1906\n",
            "Epoch 25/100\n",
            "7200/7200 [==============================] - 2s 247us/step - loss: 0.3218 - val_loss: 0.1975\n",
            "Epoch 26/100\n",
            "7200/7200 [==============================] - 2s 234us/step - loss: 0.3058 - val_loss: 0.1901\n",
            "Epoch 27/100\n",
            "7200/7200 [==============================] - 2s 265us/step - loss: 0.2871 - val_loss: 0.2021\n",
            "Epoch 28/100\n",
            "7200/7200 [==============================] - 2s 260us/step - loss: 0.2815 - val_loss: 0.1804\n",
            "Epoch 29/100\n",
            "7200/7200 [==============================] - 2s 255us/step - loss: 0.2887 - val_loss: 0.1852\n",
            "Epoch 30/100\n",
            "7200/7200 [==============================] - 2s 257us/step - loss: 0.2713 - val_loss: 0.1781\n",
            "Epoch 31/100\n",
            "7200/7200 [==============================] - 2s 243us/step - loss: 0.2607 - val_loss: 0.1715\n",
            "Epoch 32/100\n",
            "7200/7200 [==============================] - 2s 253us/step - loss: 0.2563 - val_loss: 0.1760\n",
            "Epoch 33/100\n",
            "7200/7200 [==============================] - 2s 253us/step - loss: 0.2510 - val_loss: 0.1699\n",
            "Epoch 34/100\n",
            "7200/7200 [==============================] - 2s 272us/step - loss: 0.2465 - val_loss: 0.1658\n",
            "Epoch 35/100\n",
            "7200/7200 [==============================] - 2s 262us/step - loss: 0.2491 - val_loss: 0.1653\n",
            "Epoch 36/100\n",
            "7200/7200 [==============================] - 2s 255us/step - loss: 0.2361 - val_loss: 0.1600\n",
            "Epoch 37/100\n",
            "7200/7200 [==============================] - 2s 262us/step - loss: 0.2294 - val_loss: 0.1592\n",
            "Epoch 38/100\n",
            "7200/7200 [==============================] - 2s 262us/step - loss: 0.2382 - val_loss: 0.1677\n",
            "Epoch 39/100\n",
            "7200/7200 [==============================] - 2s 258us/step - loss: 0.2212 - val_loss: 0.1586\n",
            "Epoch 40/100\n",
            "7200/7200 [==============================] - 2s 267us/step - loss: 0.2263 - val_loss: 0.1584\n",
            "Epoch 41/100\n",
            "7200/7200 [==============================] - 2s 242us/step - loss: 0.2031 - val_loss: 0.1578\n",
            "Epoch 42/100\n",
            "7200/7200 [==============================] - 2s 245us/step - loss: 0.2258 - val_loss: 0.1636\n",
            "Epoch 43/100\n",
            "7200/7200 [==============================] - 2s 228us/step - loss: 0.2088 - val_loss: 0.1568\n",
            "Epoch 44/100\n",
            "7200/7200 [==============================] - 2s 232us/step - loss: 0.1948 - val_loss: 0.1562\n",
            "Epoch 45/100\n",
            "7200/7200 [==============================] - 2s 250us/step - loss: 0.2050 - val_loss: 0.1651\n",
            "Epoch 46/100\n",
            "7200/7200 [==============================] - 2s 247us/step - loss: 0.2001 - val_loss: 0.1634\n",
            "Epoch 47/100\n",
            "7200/7200 [==============================] - 2s 257us/step - loss: 0.1953 - val_loss: 0.1686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4db432a240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW-rgdWqWiRw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7edafa02-717f-4bf3-ea87-5a95b74835f4"
      },
      "source": [
        "# GRU with glove embeddings and two dense layers\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     weights=[embedding_matrix],\n",
        "                     input_length=max_len,\n",
        "                     trainable=False))\n",
        "model.add(SpatialDropout1D(0.3))\n",
        "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
        "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
        "\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(3))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Fit the model with early stopping callback\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/100\n",
            "7200/7200 [==============================] - 6s 793us/step - loss: 1.1332 - val_loss: 1.0500\n",
            "Epoch 2/100\n",
            "7200/7200 [==============================] - 4s 546us/step - loss: 1.0780 - val_loss: 0.9561\n",
            "Epoch 3/100\n",
            "7200/7200 [==============================] - 4s 533us/step - loss: 1.0011 - val_loss: 0.8831\n",
            "Epoch 4/100\n",
            "7200/7200 [==============================] - 4s 541us/step - loss: 0.9277 - val_loss: 0.7337\n",
            "Epoch 5/100\n",
            "7200/7200 [==============================] - 4s 547us/step - loss: 0.8354 - val_loss: 0.6572\n",
            "Epoch 6/100\n",
            "7200/7200 [==============================] - 4s 546us/step - loss: 0.7598 - val_loss: 0.5399\n",
            "Epoch 7/100\n",
            "7200/7200 [==============================] - 4s 542us/step - loss: 0.6445 - val_loss: 0.4082\n",
            "Epoch 8/100\n",
            "7200/7200 [==============================] - 4s 548us/step - loss: 0.5345 - val_loss: 0.3432\n",
            "Epoch 9/100\n",
            "7200/7200 [==============================] - 4s 553us/step - loss: 0.4750 - val_loss: 0.2897\n",
            "Epoch 10/100\n",
            "7200/7200 [==============================] - 4s 538us/step - loss: 0.4398 - val_loss: 0.2734\n",
            "Epoch 11/100\n",
            "7200/7200 [==============================] - 4s 542us/step - loss: 0.4032 - val_loss: 0.2955\n",
            "Epoch 12/100\n",
            "7200/7200 [==============================] - 4s 537us/step - loss: 0.4045 - val_loss: 0.2466\n",
            "Epoch 13/100\n",
            "7200/7200 [==============================] - 4s 535us/step - loss: 0.3705 - val_loss: 0.2323\n",
            "Epoch 14/100\n",
            "7200/7200 [==============================] - 4s 543us/step - loss: 0.3373 - val_loss: 0.2190\n",
            "Epoch 15/100\n",
            "7200/7200 [==============================] - 4s 550us/step - loss: 0.3305 - val_loss: 0.2263\n",
            "Epoch 16/100\n",
            "7200/7200 [==============================] - 4s 543us/step - loss: 0.3240 - val_loss: 0.2140\n",
            "Epoch 17/100\n",
            "7200/7200 [==============================] - 4s 558us/step - loss: 0.3058 - val_loss: 0.2241\n",
            "Epoch 18/100\n",
            "7200/7200 [==============================] - 4s 536us/step - loss: 0.2989 - val_loss: 0.1893\n",
            "Epoch 19/100\n",
            "7200/7200 [==============================] - 4s 556us/step - loss: 0.2914 - val_loss: 0.1994\n",
            "Epoch 20/100\n",
            "7200/7200 [==============================] - 4s 557us/step - loss: 0.2815 - val_loss: 0.1965\n",
            "Epoch 21/100\n",
            "7200/7200 [==============================] - 4s 538us/step - loss: 0.2637 - val_loss: 0.1775\n",
            "Epoch 22/100\n",
            "7200/7200 [==============================] - 4s 560us/step - loss: 0.2500 - val_loss: 0.1876\n",
            "Epoch 23/100\n",
            "7200/7200 [==============================] - 4s 570us/step - loss: 0.2512 - val_loss: 0.2291\n",
            "Epoch 24/100\n",
            "7200/7200 [==============================] - 4s 562us/step - loss: 0.2691 - val_loss: 0.1772\n",
            "Epoch 25/100\n",
            "7200/7200 [==============================] - 4s 540us/step - loss: 0.2403 - val_loss: 0.1788\n",
            "Epoch 26/100\n",
            "7200/7200 [==============================] - 4s 536us/step - loss: 0.2352 - val_loss: 0.1684\n",
            "Epoch 27/100\n",
            "7200/7200 [==============================] - 4s 548us/step - loss: 0.2170 - val_loss: 0.1610\n",
            "Epoch 28/100\n",
            "7200/7200 [==============================] - 4s 566us/step - loss: 0.1988 - val_loss: 0.1645\n",
            "Epoch 29/100\n",
            "7200/7200 [==============================] - 4s 549us/step - loss: 0.2000 - val_loss: 0.1597\n",
            "Epoch 30/100\n",
            "7200/7200 [==============================] - 4s 542us/step - loss: 0.2069 - val_loss: 0.1662\n",
            "Epoch 31/100\n",
            "7200/7200 [==============================] - 4s 544us/step - loss: 0.2036 - val_loss: 0.1629\n",
            "Epoch 32/100\n",
            "7200/7200 [==============================] - 4s 552us/step - loss: 0.1987 - val_loss: 0.1628\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4da737bba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCBDANgiPeCU",
        "colab_type": "text"
      },
      "source": [
        "### DistilBERT + DNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1xfQOckhoy6",
        "colab_type": "code",
        "outputId": "1f60c659-0a93-4d61-b20f-8d45e42c5241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Embedding, LSTM, Dense, Input, Dropout, GRU, Conv1D, MaxPooling1D, BatchNormalization, Activation, concatenate\n",
        "from keras.layers import Bidirectional, Flatten, RepeatVector, Permute, Multiply, Lambda, TimeDistributed\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BSx72UGsw2SP",
        "colab": {}
      },
      "source": [
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# def nn_model(): \n",
        "#   units = 1024\n",
        "#   lr = 0.0005\n",
        "#   patience = 5\n",
        "#   batch = 32\n",
        "  \n",
        "#   inputs = Input(shape=(768,), dtype='float32')\n",
        "#   c = Dense(units)(inputs)\n",
        "#   x = BatchNormalization()(c)\n",
        "#   x = Dropout(0.5)(x)\n",
        "#   x = Activation('relu')(x)\n",
        "#   c = concatenate([x, c])\n",
        "\n",
        "#   def FFUnit(c):\n",
        "#     x = Dense(units)(c)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Dropout(0.5)(x)\n",
        "#     x = Activation('relu')(x)\n",
        "#     c = concatenate([x, c])\n",
        "#     return c\n",
        "\n",
        "#   for i in range(6):\n",
        "#     c = FFUnit(c)\n",
        "\n",
        "#   x = Dense(3)(c)\n",
        "#   x = BatchNormalization()(x)\n",
        "#   outputs = Activation('softmax')(x)\n",
        "\n",
        "#   model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "#   model.compile(optimizer=Adam(lr=lr),\n",
        "#                 loss=\"categorical_crossentropy\",\n",
        "#                 metrics=[\"acc\"])\n",
        "#   return model\n",
        "\n",
        "# model = KerasClassifier(build_fn=nn_model, verbose=0, epochs=100)\n",
        "\n",
        "# param_grid = dict(lr = 10 ** (np.random.rand(4) - 5), batch_size=[16,32], units=[1024, 2048])\n",
        "# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, scoring='roc_auc')\n",
        "# grid_result = grid.fit(X_val, y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlkcRUCNhv36",
        "colab_type": "code",
        "outputId": "7da51a79-019f-4b29-ab25-f2b1ee8e813d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "units = 1024\n",
        "lr = 0.0005\n",
        "patience = 5\n",
        "batch = 32\n",
        "\n",
        "\n",
        "inputs = Input(shape=(768,), dtype='float32')\n",
        "c = Dense(units)(inputs)\n",
        "x = BatchNormalization()(c)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Activation('relu')(x)\n",
        "c = concatenate([x, c])\n",
        "\n",
        "def FFUnit(c):\n",
        "  x = Dense(units)(c)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  c = concatenate([x, c])\n",
        "  return c\n",
        "\n",
        "for i in range(6):\n",
        "  c = FFUnit(c)\n",
        "\n",
        "x = Dense(3)(c)\n",
        "x = BatchNormalization()(x)\n",
        "outputs = Activation('softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train,\n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=patience, \n",
        "          batch_size=batch)\n",
        "\n",
        "cb = EarlyStopping(monitor='val_loss', \n",
        "                   mode='min', \n",
        "                   verbose=0, \n",
        "                   patience=patience,\n",
        "                   restore_best_weights=True)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr/3),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train, \n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=99, \n",
        "          batch_size=batch,\n",
        "          callbacks=[cb])\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr/6),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train, \n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=99, \n",
        "          batch_size=batch,\n",
        "          callbacks=[cb])\n",
        "\n",
        "\n",
        "print('===Evaluation===')\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 768)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_42 (Dense)                (None, 1024)         787456      input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 1024)         4096        dense_42[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 1024)         0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 1024)         0           dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 2048)         0           activation_36[0][0]              \n",
            "                                                                 dense_42[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_43 (Dense)                (None, 1024)         2098176     concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 1024)         4096        dense_43[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 1024)         0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 1024)         0           dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 3072)         0           activation_37[0][0]              \n",
            "                                                                 concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_44 (Dense)                (None, 1024)         3146752     concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 1024)         4096        dense_44[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 1024)         0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 1024)         0           dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 4096)         0           activation_38[0][0]              \n",
            "                                                                 concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_45 (Dense)                (None, 1024)         4195328     concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 1024)         4096        dense_45[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 1024)         0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 1024)         0           dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 5120)         0           activation_39[0][0]              \n",
            "                                                                 concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_46 (Dense)                (None, 1024)         5243904     concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 1024)         4096        dense_46[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 1024)         0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 1024)         0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 6144)         0           activation_40[0][0]              \n",
            "                                                                 concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_47 (Dense)                (None, 1024)         6292480     concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 1024)         4096        dense_47[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 1024)         0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 1024)         0           dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 7168)         0           activation_41[0][0]              \n",
            "                                                                 concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_48 (Dense)                (None, 1024)         7341056     concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 1024)         4096        dense_48[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 1024)         0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 1024)         0           dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8192)         0           activation_42[0][0]              \n",
            "                                                                 concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_49 (Dense)                (None, 3)            24579       concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 3)            12          dense_49[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 3)            0           batch_normalization_42[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 29,158,415\n",
            "Trainable params: 29,144,073\n",
            "Non-trainable params: 14,342\n",
            "__________________________________________________________________________________________________\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/5\n",
            "7200/7200 [==============================] - 15s 2ms/step - loss: 0.3470 - acc: 0.9401 - val_loss: 0.2330 - val_acc: 0.9689\n",
            "Epoch 2/5\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.2480 - acc: 0.9665 - val_loss: 0.2300 - val_acc: 0.9567\n",
            "Epoch 3/5\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.1986 - acc: 0.9749 - val_loss: 0.1684 - val_acc: 0.9678\n",
            "Epoch 4/5\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.1703 - acc: 0.9775 - val_loss: 0.1481 - val_acc: 0.9778\n",
            "Epoch 5/5\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.1504 - acc: 0.9786 - val_loss: 0.1516 - val_acc: 0.9689\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/99\n",
            "7200/7200 [==============================] - 15s 2ms/step - loss: 0.1299 - acc: 0.9835 - val_loss: 0.1344 - val_acc: 0.9822\n",
            "Epoch 2/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.1192 - acc: 0.9863 - val_loss: 0.1299 - val_acc: 0.9800\n",
            "Epoch 3/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.1071 - acc: 0.9885 - val_loss: 0.1175 - val_acc: 0.9822\n",
            "Epoch 4/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.1023 - acc: 0.9906 - val_loss: 0.1126 - val_acc: 0.9800\n",
            "Epoch 5/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0950 - acc: 0.9911 - val_loss: 0.1354 - val_acc: 0.9689\n",
            "Epoch 6/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0921 - acc: 0.9921 - val_loss: 0.1003 - val_acc: 0.9833\n",
            "Epoch 7/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0820 - acc: 0.9936 - val_loss: 0.0916 - val_acc: 0.9867\n",
            "Epoch 8/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0808 - acc: 0.9939 - val_loss: 0.0877 - val_acc: 0.9822\n",
            "Epoch 9/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0751 - acc: 0.9944 - val_loss: 0.0933 - val_acc: 0.9867\n",
            "Epoch 10/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0754 - acc: 0.9931 - val_loss: 0.0974 - val_acc: 0.9833\n",
            "Epoch 11/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0682 - acc: 0.9949 - val_loss: 0.0913 - val_acc: 0.9878\n",
            "Epoch 12/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0681 - acc: 0.9931 - val_loss: 0.0847 - val_acc: 0.9878\n",
            "Epoch 13/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0590 - acc: 0.9960 - val_loss: 0.0825 - val_acc: 0.9867\n",
            "Epoch 14/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0581 - acc: 0.9956 - val_loss: 0.0880 - val_acc: 0.9833\n",
            "Epoch 15/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0516 - acc: 0.9969 - val_loss: 0.0861 - val_acc: 0.9833\n",
            "Epoch 16/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0526 - acc: 0.9954 - val_loss: 0.0856 - val_acc: 0.9833\n",
            "Epoch 17/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0545 - acc: 0.9946 - val_loss: 0.0772 - val_acc: 0.9844\n",
            "Epoch 18/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0463 - acc: 0.9972 - val_loss: 0.0755 - val_acc: 0.9844\n",
            "Epoch 19/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0476 - acc: 0.9964 - val_loss: 0.0699 - val_acc: 0.9833\n",
            "Epoch 20/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0403 - acc: 0.9974 - val_loss: 0.0694 - val_acc: 0.9811\n",
            "Epoch 21/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0408 - acc: 0.9971 - val_loss: 0.0647 - val_acc: 0.9856\n",
            "Epoch 22/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0363 - acc: 0.9978 - val_loss: 0.0699 - val_acc: 0.9822\n",
            "Epoch 23/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0365 - acc: 0.9975 - val_loss: 0.0798 - val_acc: 0.9778\n",
            "Epoch 24/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0340 - acc: 0.9981 - val_loss: 0.0573 - val_acc: 0.9844\n",
            "Epoch 25/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0327 - acc: 0.9976 - val_loss: 0.0712 - val_acc: 0.9833\n",
            "Epoch 26/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0333 - acc: 0.9969 - val_loss: 0.0682 - val_acc: 0.9822\n",
            "Epoch 27/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0340 - acc: 0.9960 - val_loss: 0.0534 - val_acc: 0.9889\n",
            "Epoch 28/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0267 - acc: 0.9986 - val_loss: 0.0577 - val_acc: 0.9878\n",
            "Epoch 29/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0295 - acc: 0.9965 - val_loss: 0.0677 - val_acc: 0.9800\n",
            "Epoch 30/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0274 - acc: 0.9976 - val_loss: 0.0615 - val_acc: 0.9822\n",
            "Epoch 31/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0241 - acc: 0.9985 - val_loss: 0.0544 - val_acc: 0.9867\n",
            "Epoch 32/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0243 - acc: 0.9982 - val_loss: 0.0630 - val_acc: 0.9833\n",
            "Train on 7200 samples, validate on 900 samples\n",
            "Epoch 1/99\n",
            "7200/7200 [==============================] - 15s 2ms/step - loss: 0.0298 - acc: 0.9971 - val_loss: 0.0587 - val_acc: 0.9844\n",
            "Epoch 2/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0243 - acc: 0.9996 - val_loss: 0.0580 - val_acc: 0.9844\n",
            "Epoch 3/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0263 - acc: 0.9981 - val_loss: 0.0662 - val_acc: 0.9822\n",
            "Epoch 4/99\n",
            "7200/7200 [==============================] - 10s 1ms/step - loss: 0.0242 - acc: 0.9992 - val_loss: 0.0615 - val_acc: 0.9856\n",
            "Epoch 5/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0223 - acc: 0.9996 - val_loss: 0.0604 - val_acc: 0.9856\n",
            "Epoch 6/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0241 - acc: 0.9982 - val_loss: 0.0526 - val_acc: 0.9878\n",
            "Epoch 7/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0213 - acc: 0.9990 - val_loss: 0.0532 - val_acc: 0.9833\n",
            "Epoch 8/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0202 - acc: 0.9994 - val_loss: 0.0537 - val_acc: 0.9889\n",
            "Epoch 9/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0226 - acc: 0.9985 - val_loss: 0.0605 - val_acc: 0.9856\n",
            "Epoch 10/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0210 - acc: 0.9986 - val_loss: 0.0570 - val_acc: 0.9867\n",
            "Epoch 11/99\n",
            "7200/7200 [==============================] - 11s 1ms/step - loss: 0.0214 - acc: 0.9990 - val_loss: 0.0538 - val_acc: 0.9878\n",
            "===Evaluation===\n",
            "900/900 [==============================] - 0s 124us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05798307673798667, 0.9822222222222222]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFQ1kYGgzuF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.save('./gdrive/My Drive/DL/Style/DistilBERT.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IE9mP0d1UPX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "65dd337a-05e1-4827-93e7-5f436c6cc243"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('./gdrive/My Drive/DL/Style/DistilBERT.h5')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1lSSd-ykZPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_old_text = pd.DataFrame(X_old_text)\n",
        "X_new_text = pd.DataFrame(X_new_text)\n",
        "\n",
        "Pred_old = model.predict(X_old_text)\n",
        "Pred_new = model.predict(X_new_text)\n",
        "\n",
        "# X1_text = pd.DataFrame(X1_text)\n",
        "# X2_text = pd.DataFrame(X2_text)\n",
        "# X3_text = pd.DataFrame(X3_text)\n",
        "\n",
        "# Pred1 = model.predict(X1_text)[:, 2]\n",
        "# Pred2 = model.predict(X2_text)[:, 2]\n",
        "# Pred3 = model.predict(X3_text)[:, 2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X1GpzcNR9bH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df['Rand_donor_style'] = Pred1\n",
        "# df['Rand_117M_10000_Nabokov_All_3_style'] = Pred2\n",
        "# df['Rand_unigram_style'] = Pred3\n",
        "# df.to_csv('./gdrive/My Drive/DL/Style/rand_examples.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSGIYZt1SWc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/masked_results.csv'\n",
        "df = pd.read_csv(url).set_index('Unnamed: 0')\n",
        "\n",
        "pred_old, pred_new, pred_delta = [], [], []\n",
        "for i in range(len(X_old)): \n",
        "  if df.label.iloc[i][2] == 'A':\n",
        "    pred_old.append(Pred_old[i][0])\n",
        "    pred_new.append(Pred_new[i][0])\n",
        "  elif df.label.iloc[i][2] == 'D':\n",
        "    pred_old.append(Pred_old[i][1])\n",
        "    pred_new.append(Pred_new[i][1])\n",
        "  else:\n",
        "    pred_old.append(Pred_old[i][2])\n",
        "    pred_new.append(Pred_new[i][2])\n",
        "  pred_delta.append(pred_new[i] - pred_old[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPspREI2S7_r",
        "colab_type": "code",
        "outputId": "17408b31-e342-48f7-9fe6-f8feb82f9731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "Pred_new"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00203248, 0.00404928, 0.9939182 ],\n",
              "       [0.00727922, 0.00358726, 0.98913354],\n",
              "       [0.7721936 , 0.0139337 , 0.2138727 ],\n",
              "       [0.00891649, 0.00448649, 0.986597  ],\n",
              "       [0.1125346 , 0.00660644, 0.8808589 ],\n",
              "       [0.00478178, 0.12722215, 0.86799604],\n",
              "       [0.00129496, 0.00179181, 0.9969132 ],\n",
              "       [0.9057036 , 0.0124633 , 0.08183308],\n",
              "       [0.0044654 , 0.01962554, 0.97590905],\n",
              "       [0.01275224, 0.37084013, 0.61640763],\n",
              "       [0.00131232, 0.00184203, 0.99684566],\n",
              "       [0.00157802, 0.00203662, 0.99638534],\n",
              "       [0.06523793, 0.00728321, 0.9274789 ],\n",
              "       [0.0049958 , 0.01522104, 0.97978324],\n",
              "       [0.00574131, 0.00443773, 0.98982096],\n",
              "       [0.00687255, 0.39102003, 0.60210735],\n",
              "       [0.00370327, 0.02648717, 0.96980953],\n",
              "       [0.4538185 , 0.5200149 , 0.02616665],\n",
              "       [0.00597845, 0.89414316, 0.09987843],\n",
              "       [0.00557098, 0.21227232, 0.7821567 ],\n",
              "       [0.00246911, 0.00275852, 0.9947724 ],\n",
              "       [0.0138345 , 0.00573398, 0.9804315 ],\n",
              "       [0.99641466, 0.00144496, 0.00214028],\n",
              "       [0.99653745, 0.00132717, 0.00213536],\n",
              "       [0.9788658 , 0.01504812, 0.00608605],\n",
              "       [0.9936702 , 0.00241597, 0.0039138 ],\n",
              "       [0.9908352 , 0.00381392, 0.00535089],\n",
              "       [0.99080664, 0.00326382, 0.00592948],\n",
              "       [0.9839093 , 0.00546658, 0.0106241 ],\n",
              "       [0.78060657, 0.01258628, 0.20680715],\n",
              "       [0.98345435, 0.00465805, 0.01188755],\n",
              "       [0.9651136 , 0.00677279, 0.0281136 ],\n",
              "       [0.3471334 , 0.00445799, 0.6484086 ],\n",
              "       [0.9960854 , 0.0016397 , 0.00227486],\n",
              "       [0.9703579 , 0.00355378, 0.02608839],\n",
              "       [0.13482103, 0.01107593, 0.854103  ],\n",
              "       [0.00599374, 0.9904757 , 0.00353051],\n",
              "       [0.00191586, 0.9896975 , 0.00838658],\n",
              "       [0.9291923 , 0.06178744, 0.00902024],\n",
              "       [0.26015538, 0.72951996, 0.01032462],\n",
              "       [0.36211988, 0.6236394 , 0.01424073],\n",
              "       [0.24055268, 0.750287  , 0.00916029],\n",
              "       [0.01278337, 0.9841526 , 0.003064  ],\n",
              "       [0.11760683, 0.8728448 , 0.00954831],\n",
              "       [0.10767117, 0.8812528 , 0.01107599],\n",
              "       [0.00217144, 0.9885666 , 0.00926204],\n",
              "       [0.00230702, 0.9871459 , 0.01054701],\n",
              "       [0.9927698 , 0.00266026, 0.00456993],\n",
              "       [0.00191262, 0.9965049 , 0.00158248],\n",
              "       [0.00338391, 0.99305266, 0.00356337],\n",
              "       [0.0410355 , 0.82982755, 0.129137  ],\n",
              "       [0.09062817, 0.8946161 , 0.01475569],\n",
              "       [0.0458078 , 0.80870193, 0.14549027],\n",
              "       [0.00164791, 0.9965995 , 0.00175255],\n",
              "       [0.00889827, 0.9310496 , 0.06005207]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQLh1fA2cVQF",
        "colab_type": "code",
        "outputId": "bec3a638-6d58-457a-f169-f1658630209b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "Pred_old"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.3854805e-03, 1.4774617e-03, 9.9713707e-01],\n",
              "       [1.0117523e-03, 1.3248584e-03, 9.9766338e-01],\n",
              "       [2.9079651e-03, 2.2302193e-03, 9.9486178e-01],\n",
              "       [1.4082204e-03, 1.8278309e-03, 9.9676394e-01],\n",
              "       [1.6231856e-03, 1.9362279e-03, 9.9644059e-01],\n",
              "       [1.2254277e-03, 1.7493755e-03, 9.9702519e-01],\n",
              "       [1.1799920e-03, 1.4283181e-03, 9.9739170e-01],\n",
              "       [3.5569761e-02, 9.7493092e-03, 9.5468098e-01],\n",
              "       [3.1336043e-03, 7.9622054e-03, 9.8890418e-01],\n",
              "       [1.7892765e-03, 2.5100240e-03, 9.9570066e-01],\n",
              "       [1.3854805e-03, 1.4774617e-03, 9.9713707e-01],\n",
              "       [1.0117523e-03, 1.3248584e-03, 9.9766338e-01],\n",
              "       [2.9079651e-03, 2.2302193e-03, 9.9486178e-01],\n",
              "       [1.4082204e-03, 1.8278309e-03, 9.9676394e-01],\n",
              "       [1.6231856e-03, 1.9362279e-03, 9.9644059e-01],\n",
              "       [1.2254277e-03, 1.7493755e-03, 9.9702519e-01],\n",
              "       [1.1799920e-03, 1.4283181e-03, 9.9739170e-01],\n",
              "       [3.5569761e-02, 9.7493092e-03, 9.5468098e-01],\n",
              "       [3.1336043e-03, 7.9622054e-03, 9.8890418e-01],\n",
              "       [1.7892765e-03, 2.5100240e-03, 9.9570066e-01],\n",
              "       [1.7181025e-03, 2.0850466e-03, 9.9619687e-01],\n",
              "       [1.0057823e-03, 1.9193310e-03, 9.9707484e-01],\n",
              "       [9.9510819e-01, 1.8259594e-03, 3.0657465e-03],\n",
              "       [9.9595463e-01, 1.7217669e-03, 2.3235942e-03],\n",
              "       [9.9510819e-01, 1.8259594e-03, 3.0657465e-03],\n",
              "       [9.9595463e-01, 1.7217669e-03, 2.3235942e-03],\n",
              "       [9.9510819e-01, 1.8259594e-03, 3.0657465e-03],\n",
              "       [9.9595463e-01, 1.7217669e-03, 2.3235942e-03],\n",
              "       [9.9726260e-01, 1.2140428e-03, 1.5233783e-03],\n",
              "       [9.7003227e-01, 5.3322855e-03, 2.4635507e-02],\n",
              "       [9.9176776e-01, 3.1889551e-03, 5.0432472e-03],\n",
              "       [9.9118304e-01, 3.5566376e-03, 5.2603288e-03],\n",
              "       [9.9802750e-01, 7.4436510e-04, 1.2280226e-03],\n",
              "       [9.9561691e-01, 1.7827221e-03, 2.6004734e-03],\n",
              "       [9.9689227e-01, 1.2624987e-03, 1.8452296e-03],\n",
              "       [7.7323711e-01, 2.1403381e-01, 1.2729074e-02],\n",
              "       [1.6927321e-03, 9.9604261e-01, 2.2646475e-03],\n",
              "       [1.7298197e-03, 9.9298108e-01, 5.2891402e-03],\n",
              "       [1.1806786e-03, 9.9647850e-01, 2.3409005e-03],\n",
              "       [1.4372765e-03, 9.9714655e-01, 1.4162309e-03],\n",
              "       [1.4643536e-03, 9.9735045e-01, 1.1850906e-03],\n",
              "       [1.3079736e-03, 9.9726897e-01, 1.4230949e-03],\n",
              "       [1.8583685e-03, 9.9632382e-01, 1.8177344e-03],\n",
              "       [2.0221511e-03, 9.9638724e-01, 1.5905730e-03],\n",
              "       [4.0278542e-03, 9.9326140e-01, 2.7108039e-03],\n",
              "       [1.6927321e-03, 9.9604261e-01, 2.2646475e-03],\n",
              "       [1.7298197e-03, 9.9298108e-01, 5.2891402e-03],\n",
              "       [1.1806786e-03, 9.9647850e-01, 2.3409005e-03],\n",
              "       [1.4372765e-03, 9.9714655e-01, 1.4162309e-03],\n",
              "       [1.4643536e-03, 9.9735045e-01, 1.1850906e-03],\n",
              "       [1.3079736e-03, 9.9726897e-01, 1.4230949e-03],\n",
              "       [1.8583685e-03, 9.9632382e-01, 1.8177344e-03],\n",
              "       [2.0221511e-03, 9.9638724e-01, 1.5905730e-03],\n",
              "       [4.0278542e-03, 9.9326140e-01, 2.7108039e-03],\n",
              "       [2.1795547e-03, 9.9624926e-01, 1.5712433e-03]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHeYgeqDcpdP",
        "colab_type": "code",
        "outputId": "242fef9d-04b1-4be2-b45c-1992dee5e275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "pred_delta"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0006469997,\n",
              " 0.0062674712,\n",
              " 0.7692856,\n",
              " 0.0075082732,\n",
              " 0.11091142,\n",
              " 0.0035563535,\n",
              " 0.0001149649,\n",
              " 0.8701338,\n",
              " 0.0013317978,\n",
              " 0.010962961,\n",
              " 0.0003645725,\n",
              " 0.0007117655,\n",
              " 0.005052993,\n",
              " 0.013393214,\n",
              " 0.0025015022,\n",
              " 0.38927066,\n",
              " 0.025058856,\n",
              " 0.5102656,\n",
              " 0.88618094,\n",
              " 0.20976229,\n",
              " 0.0006734743,\n",
              " 0.0038146484,\n",
              " -0.00038100418,\n",
              " -0.00039460126,\n",
              " 0.013222156,\n",
              " 0.0006942038,\n",
              " 0.0022851413,\n",
              " 0.0036058815,\n",
              " 0.009100723,\n",
              " 0.18217164,\n",
              " 0.0068443003,\n",
              " 0.02285327,\n",
              " 0.64718056,\n",
              " -0.00032561668,\n",
              " 0.024243161,\n",
              " 0.841374,\n",
              " 0.004301003,\n",
              " 0.00018604274,\n",
              " 0.92801166,\n",
              " 0.2587181,\n",
              " 0.36065552,\n",
              " 0.2392447,\n",
              " 0.010925005,\n",
              " 0.11558467,\n",
              " 0.10364332,\n",
              " 0.0069973934,\n",
              " 0.0052578673,\n",
              " 0.0022290314,\n",
              " 0.00016624876,\n",
              " 0.002378278,\n",
              " 0.1277139,\n",
              " 0.012937957,\n",
              " 0.1438997,\n",
              " -0.0009582578,\n",
              " 0.05848083]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0byXlehgT07U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/arrow_plot_results.csv'\n",
        "df = pd.read_csv(url)\n",
        "df['donor_authorship_score'] = pred_old\n",
        "df['style_authorship_score'] = pred_new\n",
        "df['authorship_delta'] = pred_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idWN5M4qT74-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df.to_csv('./gdrive/My Drive/DL/Style/all_scores.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KxX5ljnUVgL",
        "colab_type": "code",
        "outputId": "b111eacd-be73-416c-815d-4b1c596e6d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "AUCfig = plt.figure()\n",
        "\n",
        "# Binarize the multiclass labels\n",
        "Y_bi = label_binarize(y, classes=['Austen', 'Dumas', 'Nabokov'])\n",
        "n_classes = Y_bi.shape[1]\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), pred.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "mean_tpr /= n_classes\n",
        "\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "lw = 2\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "             label='ROC curve of class {0} (area = {1:0.3f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "# plt.title('ROC curve for each class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# plt.savefig('Arrows.pdf')\n",
        "# files.download('Arrows.pdf')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAAFYCAYAAACh/d4iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd1gU1xrH8e8uXUEEAbuxd429xG40\nUWNiiQV7N/YCgopYEEVFwW5ij8auMcWe2BuW2EWNxl6Bld7b3j+4bkRF1LgMC+/nefJcZsvMbw5e\n3j1nZs9RabVaLUIIIYTIEtRKBxBCCCHExyOFXQghhMhCpLALIYQQWYgUdiGEECILkcIuhBBCZCFS\n2IUQQogsxFjpAO8qMTGJkJBopWNkaTY2OaSNM4C0s/5JG+uftHHGsLe3eu/3GEyP3djYSOkIWZ60\nccaQdtY/aWP9kzbOvAymsAshhBAifVLYhRBCiCxECrsQQgiRhUhhF0IIIbIQKexCCCFEFiKFXQgh\nhMhCpLALIYQQWYgUdiGEECIL0Wthv3nzJs2aNWPdunWvPXfy5Ek6dOhA586dWbx4sT5jCCGEENmG\n3gp7dHQ0np6e1K1b943PT5s2jYULF7Jx40ZOnDjBP//8o68oQgghRLaht7niTU1NWb58OcuXL3/t\nuYcPH2JtbU3+/PkBaNSoEX5+fpQsWfKj50hO1vLsWSR37oSQmKilceNPUj1vcvIJVlP82H77OYcT\nEoio6kB0gZx06lSeJk2Kpnqtffk1AFQKCSVGqyU2f07i45O4dGkgxsb/fkYy33ITqyl+bIuLo19k\nFJio0ZoZ0bp1KebP/zLVPnN32IHJtWBah4VzLDERbQ5jUKtYt64tdesW0r3O2P85Nh138k9SEtVD\nw0CtQpvDmKJFc3PgQPdU+7ScfBKLrbdwj4pmYWwsmBmhNVEzbtxnDBhQ7Y3nVCg4hAitFq2lCQAX\nLw7AyspMzknOSc5JzumN5/TokVOq12WFc8qMvyeWpD7Ou1BptVrte7/rPSxcuBAbGxu6d/838Pnz\n51m5cqVuCH7r1q08fPgQJyentHYDQFBQBGuOJfL3M71GFkIIITKF5UNs3/s9BnXznL29lRR1IYQQ\nWVa45h7/tb+tyLKtDg4OaDQa3XZAQAAODg7pvm/O9hDdzyuGLtT9XOPOCP4qZgPAzu1f8dXd3W98\n/65irWjdfteHxn6rZnGJbAiPAf4dvtkYF0fXiEjda9q1K8PSpV+let+L4ZsmYWEcTkjUPb59ewfq\n1y+i234xfHMzKYkyIaG6x4sXz82pU31T7fPFkNSYqCh8YmJ1j0+e3JChQ2ukeu2LIalcz4OJeOkf\n0+3bQ984JCXn9N/Pyd7eiqCgiCx1Ti/IOWWfcwoLG0dcXHyWOiclf09Fi+akbdsAflw0j++/X0Gb\nNu1ThuIxkKF4gK+++oqlS5eSL18+OnfuzJw5cyhWrNhb9zVgSTAAJjcf8f38X1Ie3NmFnXGj0yzm\n8P4FvcK9EJY8CKVcObs3Pv/PP8Go1SpMTIwwNTXCzs4CI6PXBz/i45OIjf33F2xsrCZHDpM37jMy\nMp7k5H9/FTlymKS6bv9CUlIyUVEJum21WoWlpekb9xkTk0BCQrJu28zMCDOzN3+WCw+Pw87OEo0m\n5R+vlZUpKpXK4M/pZZnlnF4U9qx0Tq9S+pwsLMwICAjPUueU2X5PxYvn0f29yCrnpNTv6fTpE0yY\n4MS9e3dQqVSMHu3CuHHuwIetx663wn716lVmzZrF48ePMTY2Jm/evDRt2pRChQrRvHlzzp49y5w5\ncwD44osv6NevX7r7fFHYl8fkeevr4gp+Qfjn2wCIjk5I8xcgUnu14Aj9kHbWP2lj/ZM2/u9CQ0OY\nOnUS69aljCCUKVMWH5+F1KpVW/eaTFXY9eFthT0hRyNCO+zI6EhZivwfNWNIO+uftLH+SRv/N/7+\nV+ncuR2BgQGYmpoyatQYhg8fjZmZWarXfUhhV+Qa+3/18tD6y9e2hRBCCENQvHgJcubMSa1adfD1\nXUjp0mU+2r4NsrC3br9LCroQQgiDkZyczPr1a/nmm7ZYW+fGwsKCX37ZRb58+VGrP+4X1Azm624L\nRl3R/fzl1SAp6kIIIQzC33/f4Ouvv8TZeQSenlN0jxcoUPCjF3UwoMJ+pXRBACol/cHU+q/PPS+E\nEEJkJnFxccyaNZ2mTetx9uxpHBzy0qhRE70f12AK+wsj4ruwxsRg7vcTQgiRDZ065UfTpvXw8ZlF\nQkICPXr04cSJs3z9dRu9H9sgr7FbN/8k/RcJIYQQCrh16yZt2rRAq9VSokRJfH0XUrduvQw7vkEV\n9kpJfwCQu0pehZMIIYQQb1aqVGm6dOlOvnz5GDXKBXNz8ww9vkENxY+I7wJA+fJvnhFOCCGEyGjP\nnj2lX7+e/PXXGd1jc+cuYty4iRle1MHAeuwv1K5dUOkIQgghsrnk5GTWrVvD1KmTCA8P48mTx+ze\nvR+VSvXGqWkzikEWdiGEEEJJt27dxNl5BKdOnQTgiy9aMGuWr6IF/QUp7EIIIcQ7io+PZ+HCucyd\nO5v4+Hjs7Ozx8vKmTZv2maKog4FdYxdCCCGUFBoaytKli4mPj6dr1x6cOHGWtm2/zTRFHaTHLoQQ\nQrxVZGQEZmbmmJiY4ODggK/vIqytrWnQoJHS0d5IeuxCCCFEGv74Yw8NGtTm++8X6R5r3fqbTFvU\nQQq7EEII8ZrAwEAGDuxN9+6defz4Efv37yM5OVnpWO9ECrsQQgjxf1qtlg0bfqJ+/Rr8+ut2cuTI\nwdSpXmzfvlMvC7bog1xjF0IIIYDQ0BD69evJsWNHAGjS5HNmz55HkSKGNY25FHYhhBACyJXLmvj4\nePLkyYOn50y+/bZTprrb/V1JYRdCCJFtXbhwjjx57ChS5BPUajVLliwnR46c5MmTR+loH8wwLhgI\nIYQQH1FkZCQTJ46nZcvPcXEZhVabshx44cJFDLqog/TYhRBCZDMHD/6Ji8toHj58gFqtply5CiQm\nJmJiYqJ0tI9CCrsQQohsQaPRMHHiOH7+eQsAlSp9iq/vAj79tKrCyT4uKexCCCGyvOjoaJo0+YyA\ngGeYm5vj6jqBQYOGYmyc9cpg1jsjIYQQ4hU5cuSgW7cenD17ljlz5lGsWHGlI+mNFHYhhBBZTmJi\nIsuWfU+RIp/QuvU3AIwZMx4jIyOD/Arb+5DCLoQQIku5cuUSo0cP5/Lli9jbO9CkyefkzJkzSw67\nv0n2OEshhBBZXnR0NLNnz+CHHxaRlJREoUKFmT17Ljlz5lQ6WoaSwi6EEMLgHTlyiDFjRnL//j3U\najXffTeEsWPdsbS0VDpahjO4wh5X8AulIwghhMhEEhIScHUdzf379yhXrgJz5y6kWrUaSsdSjMEV\n9vDPtykdQQghhMK0Wi0JCQmYmppiYmLCnDnzOX/+L4YMGZFlJpr5UAZX2IUQQmRvDx8+wNV1NIUK\nFWH27LkANGjQiAYNGimcLHOQueKFEEIYhKSkJJYtW0KDBrU5cOBPfvvtZ4KDnysdK9ORwi6EECLT\n8/e/yldfNcPdfRzR0VF88007jh07i62tYS/Yog8yFC+EECLT0mq1zJjhyaJF80hMTCR//gLMmuVL\nixatlI6WaUmPXQghRKalUql4+PABSUlJ9O07gOPHz0hRT4f02IUQQmQqoaEhaDQaSpYsBYCn50z6\n9h1AzZq1FU5mGKTHLoQQIlPQarX89tt26tWrSb9+PYiPjwfAzs5Oivp7kMIuhBBCcY8fP6JnT0cG\nDOhNUFAgVla5CAkJUTqWQZLCLoQQQjHJycmsXLmM+vVrsW/fHqyscuHtPZfff99L3rx5lY5nkAzq\nGrtMJyuEEFlLjx6d+fPPfQC0bNmamTPnkD9/AYVTGTaD6rHLdLJCCJG1tGr1NXnz5mPVqnWsWbNB\nivpHYFA9diGEEIbt1Ck/7t27g6NjNwC6du3BN9+0xcoql8LJsg6D6rEb+8vUgUIIYYjCw8NwcRnN\nN998iavraO7evQOkfE9divrHZVCF3abjTqUjCCGEeE+7du2gfv1arFmzEhMTE4YMGSFD7nokQ/FC\nCCH04tmzp4wbN4bdu3cAUL16TebOXUTZsuUUTpa1SWEXQgihF05Ow9m//w9y5rTE3X0yvXv3x8jI\nSOlYWZ5BFfaE8rZKRxBCCPEWWq0WlUoFwJQp0zEzM2fatJkULFhI4WTZh0FdYw/d9rXSEYQQQrxB\nfHw8Pj6z6N27G1qtFoDSpcuwevU6KeoZTK89di8vLy5duoRKpcLNzY3KlSvrnlu/fj2///47arWa\nihUrMmHCBH1GEUIIoSdnz57G2XkEN25cB+DcubPUqFFL4VTZl9567GfOnOH+/fts3ryZ6dOnM336\ndN1zkZGRrFy5kvXr17Nx40Zu377NxYsX9RVFCCGEHkRGRjB+/Bhat/6CGzeuU6xYcbZv3ylFXWF6\n67H7+fnRrFkzAEqUKEFYWBiRkZFYWlpiYmKCiYkJ0dHR5MiRg5iYGKytrfUVRQghxEe2a9cuBg78\njidPHmNsbMzQoSNxcnLFwsJC6WjZnt4Ku0ajoUKFCrptW1tbgoKCsLS0xMzMjKFDh9KsWTPMzMz4\n6quvKFasmL6iCCGE+Mj8/Px48uQxVatWw8dnIRUrVlI6kvi/DLsr/sXNFJAyFL906VL27t2LpaUl\nvXr14saNG5QtW/at+7C3t9J3zGxP2jhjSDvrn7Txx6XVarl//z5FixYFwN3dnaJFi9KnTx/5Clsm\no7fC7uDggEaj0W0HBgZib28PwO3btylcuDC2tilfX6tRowZXr15Nt7DHDNlHpMdn+oqc7dnbWxEU\nFKF0jCxP2ln/pI0/rjt3/sHZeSQ3b/7NiRNnyZ3bBnt7K9q06UxwcLTS8bK0D/mAqreb5+rVq8e+\nfSlL8fn7++Pg4IClpSUABQsW5Pbt28TGxgJw9epV3afAt7HYektfcYUQQrwiISGB+fN9aNSoLidO\nHCM5OYlbt24qHUukQ2899mrVqlGhQgUcHR1RqVRMnjyZ7du3Y2VlRfPmzenXrx89e/bEyMiIqlWr\nUqNGDX1FEUII8Z4uXDjH6NHDuXbtKgCdOnXBw8OLPHnyKJxMpEev19jHjBmTavvloXZHR0ccHR31\neXghhBAfYNGi+UybNpnk5GSKFCnKnDnzaNy4qdKxxDsyqJnnIqbUVTqCEEJkeZUqVUalUjF06EiO\nHj0lRd3AGNRc8bGdSisdQQghspygoCAOHPgDR8duADRq1IQzZy5RuHARhZOJD2FQPXYhhBAfj1ar\nZdOm9dSvX4MRIwZz+vQp3XNS1A2XQfXYhRBCfBx3797BxWU0R48eAqBhwybkzZtX4VTiY5DCLoQQ\n2UhiYiI//LCY2bO9iImJwcbGhqlTZ9CpUxfdcqvCsElhF0KIbGT2bC/mzp0DwLffdsLTcyZ2dnYK\npxIfkxR2IYTIRvr3H8zBgwcYN24Cn3/+hdJxhB4Y1M1z9uXXKB1BCCEMyuHDB+nZswsJCQkA2Nvb\n88cfh6WoZ2EGVdiFEEK8m+Dg5wwb9h2dOrVl795drFv3b8dIrqVnbTIUL4QQWYhWq2X79q24u4/l\n+fPnmJubM2bMOLp376V0NJFBpLALIUQW8fDhA1xcRnHw4H4A6tdvyJw58yhevKTCyURGMqjCHnRN\nPnEKIURajh49zMGD+7G2zo2Hx3S6dOkuw+7ZkEEVdiGEEKmFhYVibZ0bgK5de/Ds2VO6d+8tk81k\nY3LznBBCGKCYmBimTZtCtWoVuXfvLpByU5yz81gp6tmcFHYhhDAwx48fpXHjuixY4EtkZASHDx9U\nOpLIRGQoXgghDERISDAeHhPZsOEnAMqWLYePzwJq1qytcDKRmUhhF0IIA3D06GEGDeqHRhOEqakp\no0e7MHz4aExNTZWOJjIZgxqKN99yU+kIQgihiLx58xEWFkrt2nU5dOgkzs5jpaiLNzKowm41xU/p\nCEIIkSGSkpLYtWsHWq0WgDJlyrJ7935++20PpUqVVjidyMwMqrALIUR2cP36NVq3/oI+fbrxyy/b\ndI9/+mlV1Gr5sy3eTq6xCyFEJhEbG8u8eXNYuHAuCQkJ5M2bDysrK6VjCQNjUIU9pmMppSMIIYRe\nnDp1Eien4fzzzy0AevXqx8SJU8iVy1rhZMLQGFRhj/T4TOkIQgjx0e3Zs4tevboAUKpUaXx8FlCn\njvy9Ex/GoAq7EEJkRU2afE65cuVp1eprRo0ag5mZmdKRhAGTuzCEECKDPXv2lNGjhxEaGgKAubk5\n+/cfY+zYCVLUxX8mPXYhhMggycnJrF27Gk/PyUREhGNmZsbMmT4AmJiYKJxOZBVS2IUQIgPcvPk3\nzs4jOH06ZT6OFi1aMWKEk8KpRFYkhV0IIfQoPj6eBQt8mTdvDvHx8Tg45GXGjNm0bt1G1koXemFQ\n19hzd9ihdAQhhHgv586dxdvbi/j4eLp378Xx42f4+uu2UtSF3rxTjz0kJIRHjx5RqVIlkpOTFZv5\nyORasCLHFUKI9xEfH6+bx71u3Xo4ObnSoEEj6tVroHAykR2kW6F37txJ586dGT9+PACenp5s3bpV\n78GEEMIQ7d27mzp1qnLmzGndY+PGuUtRFxkm3cK+evVqfvvtN2xsbAAYO3YsW7Zs0XswIYQwJAEB\nAfTr15OePR159OghP/64QulIIptKdyjeysoKCwsL3ba5ubliX8sI2dpakeMKIURatFot69evxcNj\nImFhoeTIkRM3t4n06/ed0tFENpVuYbexseGXX34hLi4Of39/du/eja2tbUZke01ihTyKHFcIId7k\n0aOHDB8+iBMnjgHQrNkXzJrlS+HCRRROJrKzdIfiPTw8uHLlClFRUbi7uxMXF8f06dMzIpsQQmRq\nZmbmXLt2FTs7O374YSXr12+Voi4Ul26P/dixY0yaNCnVYxs3bqRLly56CyWEEJnV5csXKVeuAiYm\nJtjb27NmzUZKly6Dra2MKIrMIc3Cfu3aNfz9/Vm1ahUxMTG6xxMTE1m8eLEUdiFEthIZGcnMmZ4s\nX/4Dbm6TGDnSGUBWYROZTpqF3czMjOfPnxMREcG5c+d0j6tUKlxdXTMknBBCZAb79+/D1dWJR48e\nYmRklKqzI0Rmk2ZhL1GiBCVKlKBOnTpUqVIl1XP79u3Te7A3MfZ/LjfQCSEyTFBQEO7urvzyy88A\nVK5chblzF1Kp0qcKJxMibeleY3dwcMDb25uQkJTlBePj4zl9+jRffvml3sO9yqbjToKu9crw4woh\nsp87d/6hZcvPCQkJwcLCgrFj3Rk4cDDGxrLEhsjc0r0r3tXVldy5c3Px4kUqVqxISEgI3t7eGZFN\nCCEUU7RoccqVq0CjRk04cuQUQ4YMl6IuDEK6hd3IyIiBAwdiZ2dHt27d+P7771m/fn1GZBNCiAyT\ncmPwAu7duwuAWq1m7dqNbNnyK0WLFlM4nRDvLt3CHhcXx7Nnz1CpVDx8+BBjY2MeP36cEdlek1Be\nmYlxhBBZ2+XLF/nyyyZ4eLjj4jIKrVYLQK5c1rIKmzA46Y4r9e/fHz8/P/r160ebNm0wMjKidWtl\npnYN3fa1IscVQmRNUVFReHt7sXTpYpKTkylcuAiDBw+XYi4MWrqFvVmzZrqfz5w5Q1RUFNbW1noN\nJYQQ+nbo0AFcXEbz4ME91Go13303lLFjJ2Bpaal0NCH+kzSH4pOTk9m0aROenp7s3LkTAGNjY0xN\nTfHw8MiwgEII8bE9e/aUHj068+DBPSpUqMSePQfw9JwhRV1kCWn22D09PQkLC6NKlSps2rSJkJAQ\nSpYsyaRJk1L14oUQwhC8uG6uUqnIly8/rq5ugIrBg4cptmKlEPqQZmG/fv06mzZtAqBDhw40adKE\nggULMnfuXCpWrJhhAYUQ4r968OA+Li6jcHTsRrt2HQAYMcJJ4VRC6Eeahf3lT7A5cuSgWLFirF+/\nHiMjo3feuZeXF5cuXUKlUuHm5kblypV1zz19+hQnJycSEhIoX748U6dO/cBTEEKIN0tKSmL58u+Z\nOXMa0dHRPHz4gDZt2qNWp/uFICEMVpr/ul+9K9TU1PS9ivqZM2e4f/8+mzdvZvr06a8t9Tpz5kz6\n9u3Ltm3bMDIy4smTJ+nu03LyyXc+vhAie7t69QotWzZl0iQ3oqOjadu2Pb/+ukeKusjy0uyxBwYG\nsm3bNt12UFBQqu0OHTq8dcd+fn66a/ElSpQgLCyMyMhILC0tSU5O5ty5c/j6+gIwefLkdwprsfUW\nkR6ykpIQIm2xsbGMH+/F7NmzSUpKokCBgnh7+/LFFy2VjiZEhkizsFetWjXVqm5VqlRJtZ1eYddo\nNFSoUEG3bWtrS1BQEJaWlgQHB5MzZ05mzJiBv78/NWrUwNnZ+b+chxBCACk3yf38888kJyfTv/93\nuLlNwtLSSulYQmSYNAv7jBkzPuqBXtyR+uLngIAAevbsScGCBRk4cCCHDx+mcePG6e7H3l7+D6pP\n0r4ZQ9r54woODsbIyOj/c2xYsWbNGlQqFXXq1FE6WpYm/44zJ72taODg4IBGo9FtBwYGYm9vD4CN\njQ0FChSgSJEiANStW5dbt26lW9gjptQlNihCX5GzPXt7K4KkffVO2vnj0Wq1/PbbdtzcXGnZsjU+\nPvOBlL8pQUER0s56JP+OM8aHfHjS210k9erV063b7u/vj4ODg27yB2NjYwoXLsy9e/d0zxcrlv4i\nC7GdSusrrhDCwDx69JDu3TsxcGAfNJogbt++RXx8vNKxhFCc3nrs1apVo0KFCjg6OqJSqZg8eTLb\nt2/HysqK5s2b4+bmxrhx49BqtZQuXZqmTZvqK4oQIgtJSkpi9erlTJ8+laioSHLlsmbyZE+6desp\nd7wLwTsU9hs3buDmlvJ1kb1797J48WLq16/Pp59+mu7Ox4wZk2q7bNmyup8/+eQTNm7c+AGRhRDZ\nVUxMDO3bt+bcubMAtG7dhhkzZpM3bz6FkwmReaT78Xbq1Kl4eXnpro+3atXqo99YJ4QQ78LCwoLi\nxUuQL19+fvxxA6tW/SRFXYhXpNtjNzY2TtXTLlasGMbGehvBF0KIVPz8TmBubk7VqtUBmD59Fmq1\nmly5ZJVJId4k3R67sbExDx8+1M1Ed+TIkVRfXRNCCH0ICwvF2Xkkbdq0ZOTIIbob43LntpGiLsRb\npNv1Hjt2LEOGDOHu3btUr16dggUL4u3tnRHZXmNffg1B13opcmwhRMbQarXs3Pk7bm4uBAQ8w8TE\nhNat2ygdSwiDkW5hNzExYceOHQQHB2NqairrFQsh9Obp0yeMHevM3r27AKhZsza+vgspU6ZsOu8U\nQryQbmEfPHgwVlZWfPPNN7Ru3TojMgkhsqHExES+/vpLHjy4j6WlFRMnetCrV1/5CpsQ7yndwr5v\n3z6uXr3Knj17cHR0pFixYrRp04ZWrVplRD4hRDZhbGyMs/NY9uzZxcyZcyhQoKDSkYQwSCrte9wJ\nFxQUxJIlS9i6dStXr17VZ67XDFgSjFdHk/RfKD6YTBGZMaSdU8TFxbFggS+WllYMHjwM+HdNiVeX\njX5f0sb6J22cMT5kStl0e+yBgYH88ccf7N27l+DgYFq1asWuXbs+KKAQQgCcOXMaJ6dh3Lz5NxYW\nFnTs6Iidnd1/LuhCiHco7N9++y2tWrVi7NixVKpUKSMyCSGyqIiIcKZNm8KPP65Eq9VSvHgJfHwW\nYGdnp3Q0IbKMNAt7YGAgDg4OrF27VjchzcOHD3XPFy5cWP/phBBZxp49uxg3zpmnT59gbGzM8OGj\nGD3aFXNzc6WjCZGlpFnYZ82ahY+PD/369UOlUqWalEalUnHgwIEMCSiEMHxarZYffljE06dPqFat\nOr6+iyhfvoLSsYTIktIs7D4+PgAsX76cEiVKpHruwoUL+k0lhDB4ycnJREZGkCuXNSqVCh+fBRw6\ntJ++fQdiZGSkdDwhsqw0vyAaHh7OgwcPcHNz4+HDh7r/7ty5w7hx4zIyo475lpuKHFcI8X7++ecW\n7dp9Rb9+PXWjfSVLlmLAgMFS1IXQszR77BcuXGDNmjVcv36dXr3+ncZVrVZTv379DAn3KqspfsR2\nKq3IsYUQ6YuPj2fx4vn4+noTFxeHnZ0djx49pHDhIkpHEyLbSLOwN2rUiEaNGrFx40a6dOmSkZmE\nEAbo3LmzODkN5/r1awA4OnZjypRp2NrmUTiZENlLmoX9559/5ttvvyUgIID58+e/9vzIkSP1GkwI\nYTimTHHn++8XotVq+eSTosyZM59GjZooHUuIbCnNwv5ifubMtPZ6TMdSSkcQQryBhYUFarWawYOH\nM2bMOHLkyKF0JCGyrXeaUjYyMhJLS0s0Gg337t2jWrVqGb4wg0wpq38yRWTGyArtHBgYyN27d6hd\nuw6QMj3srVs3qVgxc0xilRXaOLOTNs4YHzKlbLrV2dPTkz179hAaGoqjoyPr1q1jypQpH5JPCGHg\ntFotGzeuo379GvTp042QkGAAzMzMMk1RFyK7S7ewX7t2jY4dO7Jnzx7atWvHvHnzuH//fkZkE0Jk\nInfu3KZDhzaMHDmE0NBQKlasRGxsrNKxhBCvSLewvxipP3z4ME2bNgVSvtIihMgeEhISWLBgLo0b\n1+XYscPY2tqyePEyNm/+hfz5CygdTwjxinTvjCtWrBitWrXC1taWcuXK8euvv2JtbZ0R2YQQmcCg\nQf3YseNXADp06MzUqTNk0RYhMrF0C/u0adO4efOmblrZkiVL4u3trfdgQojMoU+f/ly6dBFvb1+a\nNm2mdBwhRDrSLeyxsbEcPHiQ+fPno1KpqFKlCiVLlsyIbK/J3WEHodu+VuTYQmQXhw4d4Ny5s4wZ\nkzJ1dP36DfHzO4eJiXwrRQhDkO419okTJxIZGYmjoyOdOnVCo9Hg7u6eEdleY3ItWJHjCpEdPH/+\nnCFDBtC5czu8vb04d+6s7jkp6kIYjnR77BqNBl9fX912kyZN6NGjh15DCSEyjlarZdu2zUycOI7g\n4GDMzc1xcXGjcuUqSkcTQnyAdAt7TEwMMTExWFhYABAdHU1cXJzegwkh9O/+/Xu4uo7m0KEDADRo\n0IjZs+dRvHiJdN4phMis0nmNPMwAACAASURBVC3snTt3pmXLllSsWBEAf39/xeaJD9naWpHjCpFV\nzZs3h0OHDpA7d26mTp1B585dUalUSscSQvwH6Rb2Dh06UK9ePfz9/VGpVEycOJG8efNmRLbXJFaQ\nVaKE+K8SEhJ018zd3T1Qq9WMHeuOg4ODwsmEEB/DWwv7kSNHuHPnDtWrV6dZM/maixCGLDo6mjlz\nZnL48EH27j2IqakpefLkwcdngdLRhBAfUZp3xS9cuJDvv/+ewMBA3N3d+f333zMylxDiIzp69DCN\nG9dl0aJ5+Ptf4fjxo0pHEkLoSZo99uPHj7N+/XqMjY2JiIhg+PDhfPPNNxmZTQjxH4WEBDNlijsb\nN64DoFy58vj6LqR69ZoKJxNC6Euahd3U1FS3FruVlRVJSUkZFkoI8d/t2rUDF5dRaDRBmJmZ4eTk\nytChIzE1NVU6mhBCj9Is7K/eGZsZ7pQ19n8uN9AJ8Y7CwkLRaIL47LP6+PjMp0SJUkpHEkJkgDQL\n++3bt3F1dU1zW4n54m067iToWq8MP64QhiApKYmrVy/z6adVAejSpTs2NrZ8+WVL1Op0J5kUQmQR\naRb2MWPGpNquW7eu3sMIIT7MtWv+ODkN49o1fw4f9qN48RKoVCpatvxK6WhCiAyWZmFv165dRuYQ\nQnyA2NhYfH29WbRoHomJieTPX4DAwACZOU6IbCzdCWoyk4TytkpHECLTOHnyOE5Ow7lz5zaQsryq\nu/sUrKxyKZxMCKEkgyrssmSrECmWLl3MxInjAShdugw+PgupXbuOwqmEEJnBO91RExISwpUrVwBI\nTk7WayAhRPqaNm2OpaUVLi7jOXDguBR1IYROuoV9586ddO7cmfHjU3oHnp6ebN26Ve/BhBD/evr0\nCb6+3mi1WgBKlSrNhQv+uLiMx8zMTOF0QojMJN3Cvnr1an777TdsbGwAGDt2LFu2bNF7MCFEygjZ\nqlXLqVevJjNnTmPbts2656ytcyuYTAiRWaV7jd3Kykq3FjuAubm5bmUoIYT+/P33DZychnP27GkA\nWrZsTf36DRVOJYTI7NIt7DY2Nvzyyy/ExcXh7+/P7t27sbWVu9OF0Je4uDjmzZvDggW+JCQk4OCQ\nl5kzfWjdWtZqEEKkL92heA8PD65cuUJUVBTu7u7ExcUxbdq0jMj2GsvJJxU5rhAZafXq5fj4zCIh\nIYEePfpw4sRZKepCiHeWbo89V65cTJo0KSOypMti6y0iPT5TOoYQH51Wq9Wtx9CnzwBOnjzO4MHD\nqVu3nsLJhBCGJt3C3qhRozcuAHP48GF95BEi29mzZxc+PrPYuvVXbGxsMTMzY+3aTUrHEkIYqHQL\n+4YNG3Q/JyQk4OfnR1xc3Dvt3MvLi0uXLqFSqXBzc6Ny5cqvvcbHx4eLFy/y008/vUdsIQxfQMAz\nxo93YefO3wD48ceVjB7tonAqIYShS7ewFyxYMNV20aJF6devH717937r+86cOcP9+/fZvHkzt2/f\nxs3Njc2bN6d6zT///MPZs2ff+S77iCmyEI0wfMnJyaxdu5qpUycRHh5GzpyWTJgwiT59BigdTQiR\nBaRb2P38/FJtP3v2jAcPHqS7Yz8/P5o1awZAiRIlCAsLIzIyEktLS91rZs6cyejRo1m0aNE7hY3t\nVPqdXidEZnX79i06dBjN0aNHAWje/EtmzfKlUKHCCicTQmQV6Rb2JUuW6H5WqVRYWlri4eGR7o41\nGg0VKlTQbdva2hIUFKQr7Nu3b6dWrVqvjQgIkZU9efKEo0ePYmdnj5eXN23atH/jPSxCCPGh0i3s\n48aNS1WgP9SLqTABQkND2b59O6tXryYgIOCd92Fvb/Wfc4i3kzb++O7du0fRokUBaN++NatXr+ab\nb76R+SD0TP4t65+0ceaUbmGfNWsWa9eufe8dOzg4oNFodNuBgYHY29sDcOrUKYKDg+nWrRvx8fE8\nePAALy8v3Nzc3rrPoKCI984h3p29vZW08UcUGRmBl9dUVq9ewS+/7KJOnZSvavbu3ZugoAhpaz2S\nf8v6J22cMT7kw1O6hb1AgQL06NGDTz/9NNVNbiNHjnzr++rVq8fChQtxdHTE398fBwcH3TB8ixYt\naNGiBQCPHj1i/Pjx6RZ1IQzJH3/sYexYZx4/foSRkRFXr17WFXYhhNCndAt7oUKFKFSo0HvvuFq1\nalSoUAFHR0dUKhWTJ09m+/btWFlZ0bx58w8KK0RmFxgYiLu7K7/+uh2AKlWq4uOzkEqVXv+qpxBC\n6INK+/LF75f8/vvvfPNN5pnGcsCSYJZP3kDQtV5KR8myZGjtvzl58ji9e3clNDSUHDlyMG6cO/37\nD8LYOPXnZ2ln/ZM21j9p44zxIUPxac4Vv23btv8URojspnTpsqjVapo0+ZyjR08zaNCw14q6EELo\nm/zVEeIDJSQksGHDT3Tp0h1TU1Ps7Oz4448jFC5cRL7CJoRQTJqF/cKFCzRu3Pi1x18sViFzxYvs\n7OLF84wePRx//ysEBz/XTQVbpMgnCicTQmR3aRb28uXL4+vrm5FZ0iXX14XSoqKimDlzGsuXf09y\ncjJFihSlatXqSscSQgidNAu7qampzAonxEsOHvwTF5fRPHz4ALVazZAhI3BxGU/OnDmVjiaEEDpp\nFvY3rcQmRHZ14sQxHB2/BaBSpU/x9V3Ap59WVTiVEEK8Ls3C7uIiy0cK8cJnn9WnefMvqVu3PoMG\nDZW73YUQmVaaX3cTIju7f/8ePXs6cufObSBlAaR167YwbNhIKepCiExN/kIJ8ZLExESWLl2Ct/d0\nYmJiMDIyZvXqdQDyFTYhhEEwqB67+ZabSkcQWdiVK5do0aIpHh7uxMTE0L59B7y95yodSwgh3otB\nFXarKX5KRxBZUHR0NFOnTuKLLxpz+fJFChUqzIYNW/nhh1W6FQmFEMJQGFRhF0IfHj9+xLJlS0hO\nTmbgwMEcPXqaZs2+VDqWEEJ8ELnGLrKl8PAwrKxyoVKpKFWqNLNm+VK2bDmqV6+pdDQhhPhPDKrH\nHtOxlNIRhIHTarVs376VOnWq8vPPW3SPd+vWU4q6ECJLMKjCHunxmdIRhAF7+PABXbt2YNCgfmg0\nGvbs2aV0JCGE+OgMqrAL8SGSkpJYtmwJDRrU5sCBP7G2zo2v70KWL/9R6WhCCPHRyTV2kaU9efKY\nvn27c/78OQC++aYd06d7kzdvXoWTCSGEfkhhF1marW0ewsLCyJ+/ALNm+dKiRSulIwkhhF5JYRdZ\njp/fCcqVK0/u3DaYm5uzZs1G8ufPj5VVLqWjCSGE3sk1dpFlhIaG4OQ0nDZtWuLhMVH3eOnSZaSo\nCyGyDYMq7Lk77FA6gsiEtFotO3b8Sr16NVm3bg2mpqYUKlQYrVardDQhhMhwBjUUb3ItWOkIIpN5\n8uQx48Y5s3fvbgBq166Lj88CSpcuo3AyIYRQhkEVdiFeFhAQQIMGtYmICMfKKhcTJ3rQs2cf1GqD\nGogSQoiPSgq7MFh58+blq6++Jjw8nBkzZpM/fwGlIwkhhOIMqrCHbG2tdAShoLi4OObNm0Pjxp9T\nu3YdAObMmY+pqanCyYQQIvMwqMKeWCGP0hGEQk6d8sPZeTi3bt1k587fOHzYDyMjIynqQgjxCoMq\n7CL7CQ8Pw9NzCmvWrASgZMlSeHvPxcjISOFkQgiROUlhF5nW7t07GTfOmWfPnmJiYsLw4aMZNWoM\n5ubmSkcTQohMSwq7yJTCw8NwchpGcHAw1avXxNd3IeXKlVc6lhBCZHpS2EWmkZycTHJyMsbGxuTK\nZc2MGXMIDn5O7979ZehdCCHekUEVdmP/53IDXRZ169ZNnJ1H0LRpM0aNGgNAu3YdFE4lDM3z5xpW\nrlzK7NkzlY6id8OGDSQ2NjbVpSln53EUK1YcgD/+2MvmzesxNjYmMTGRHj1607jx5wAkJiayfPn3\nnDnjh7m5BSYmJowcOYYSJUoqci4v/Prrz0RFRdKtWy9FcwQEPMPNzYWqVaszbNio156/desmPj4z\nUamgRIlSjBkzHoANG9Zy6NB+QEXfvgOoVasuLi4jcXWdQL58+TMsv0EVdpuOOwm6puwvXHxc8fHx\nLFw4l7lzZxMfH8/jx48YPHg4ZmZmSkcTBihPHjtcXScoHSPDuLlNonjxlGJ8/vxfzJs3m/nzv+fq\n1cts2bKBuXMXkSuXNVFRkYwZMxJLSytq1KjFhg1riYyMYNWq9ahUKq5cuYSb2xjWr9+GsbEyZSEk\nJJjff/+FFSvWKnL8l82YMZXq1WuSnJz8xucXLPBh5EhnypWrwJQpE/DzO8EnnxRl//4/WLp0NZGR\nkQwd2p+ffqrLoEHD8PX1xtt7boblN6jCLrKWs2dP4+w8ghs3rgPQrVtPJk/2lKJuQOzLr0m1ndYH\nb/MtN7Ga4qfbjulYikiPz977eLt37+DixfOEhoZy9+4dBg4czP79+7h37y6TJk3D1tYWd/ex/P77\nr5w9e4qlS5egVqtp1uwLOnXqiqNjO+rUqYeNjQ0tW7ZmxoypJCQkoFarGTduIgUKFEx1vLNnT7Ni\nxQ+YmJhgZWXF1KkzmTRpPJ07d6VKlWrExcXSrVtHNm/+lRUrfuDy5YskJyfRvn0nmjdvwfTpUzA2\nNiE8PBQ3t8l4eLgTExNDbGwso0e7UL58Rfbu3cWGDWtxcMiLtXVuqlevyZdftsLbezpPnjwmMTGR\n/v0HUb16zbe2TYUKFXn48AEAW7dupF+/geTKZQ1AzpyWfPfdUDZsWEuNGrX49defWbNmEyqVCoBK\nlT5lxYqfXivqe/fuYtu2zahUKhwdu/H551/w1Vefs2vXAQDc3V1p374TFy6c48mTxzx9+gRr69zv\n3D4v++237bRo0Qq1Wk1gYACenpOAlNEFd3cPChYshKNjO0qXLkutWrWpUKEyc+d6o1KpyJEjB25u\nU7CysmLhQl+uXfMnPj6etm2/5euv2+qOkZSUxMiRg1MdN2/efEycODXVY15eszl8+CB37tx+rZ0T\nEhJ4+vQJ5cpVAKBevQb89dcZnj/XUKfOZ5iYmGBjY0O+fPm5d+8upUuXJSIinEePHlKoUOG3/g4/\nFinsIsPFxcUxZcoEVq1ajlarpVix4vj4LKB+/YZKRxMG4OHDByxZsoIdO35l3bofWbVqPXv27GD/\n/n106tQFSFkYyMdnFt9/v4pcuXIxfrwzbdq0JzExkTp1PqNOnc/w8vKgdes2fP75Fxw6tJ9Vq5bh\n7u6R6lgRERFMnjyNAgUK4uk5idOn/WjUqAknThyjSpVqnD17mpo163D16mUCAp6xePFy4uPj6du3\nOw0bNgYgV65cjB07gQcP7tO6dVsaNmzMuXNnWb9+DZ6es1i6dDErV/6EhUUOevbsTPXqNfnzz73k\nyWPH+PGTCA0NZeTIQaxZs+mt7XLo0AHKlCkLwP379ylVqmyq50uVKs2DB/eJjIzE1NQMKyurVM+/\nuh0dHcWPP65gzZqNxMcnMH36ZD7//Is0j5+YmMCSJSvYu3fXO7ePmdm/lxHOn/+LoUNThr2fP9fQ\np88AqlWrwc6dv7F9+1aGDx/NkyeP8fKaQ/HiJRg5cjAuLm4ULlyE7du3sn37Fhwdu5MvXwGGD3ci\nLi6WTp3apirsRkZGLFq07K3tCJAjR840nwsLC03VVjY2tjx/rsHa2prcuW1eetyG5881lChRkipV\nqnH+/F9S2N8kobyt0hHER2BqasqNG9dRq9UMGzYKJydXLCwslI4lDETZsuVRqVTkyWNHiRKlMDIy\nwsYmD1FRl3SvCQ4OxtTUFBublD+03t7zdM+VL5/S0/r77+sMGjQMgGrVavDjjyteO1bu3LmZNWsa\nSUlJPHnymOrVa9KgQWM2bFjL0KEjOXbsCJ9//gVXrlzC3/8Kw4YNBECrTUaj0aQ6nq1tHtasWcHG\njT+RkJCAubk5YWGh5MyZE1vblHuHXvTKr169zKVLF7h8+SKQ8mE4ISEBExOTVPm8vKZibm6ORqOh\nQIECuLlNAUClguTkpFSv1WrRraPw6nNvcu/eXYoUKYqZmTlmZubMnOn71tf/24Nt+M7tU7BgId37\nNZogHBwcdG01b94cVq5cSkREOGXKlAPA3NyC4sVLAHDtmj+zZk0DUnrR5cqVx8zMjPDwMAYN6oux\nsTGhoSHpnud/ldYqki8/bG/vQGBggN6zvGBQhT1029dKRxAfKCAggMTEBAoWLIRKpcLXdyFRUVFU\nrFhJ6WjCwLz8DYmXf375D6xarSY5+c1/cI2NXxRHle49CQmJqFRqrl69zA8/LAJg8uRpzJjhyezZ\n8yhatBi+vrOAlJ6tnZ0DDx7c4+rVy7i4uHH37m1at25Djx590jzeli0bsLNzYOJET27cuMaiRfPQ\narW64XBA97OxsQk9e/Z9bbj6VS+usZ84cYwdO37Bzs4OgCJFinLjxnUcHPLqXnvr1t8ULVocS0tL\nEhMTCQ5+rvtAAfD33zcoXbqMLoNabYRW++ZrzC8kJibqfn7xoeN92ye1lGOvXLmU2rXr0LZtBw4d\n2s/Jk8f/f4x/S5a5uTkLFy5N1X4XLpzj/Pm/WLRoGcbGxjRv3iDV3t91KP5tcue2ISwsTLet0QRh\nZ2ePnZ09Dx7c1z0eFBSo+31kNFkGS+iVVqtl/fq11K9fkxEjhuj+kBYrVlyKehYQdK1Xqv/SEtup\ndKrXfcj19fdhY2NDcnISQUGBaLVaXF1HERERkeo15cqV5/z5vwC4ePEcZcuWo2LFyixatIxFi5Zh\nb+9AVFQkefPmIyIigvPnz5GQkABAw4aNWbNmFRUqVMLY2Jjy5Sty4sQxkpOTiYuLY+5c79cyhYWF\n6nqoR44cIjExkVy5rAkPDyM8PJy4uFguXDgHQPnyFTl+/AiQclPZ0qWL33q+9eo1ID4+XlcAO3bs\nwqpVywgJSemxRkdHsWzZEjp37grAt992YsECX11hvnz5Il5eU4iPj9ft85NPivLgwX2io6OJi4tj\n1Kghug8isbGxxMTEcPPm32/M8yHtY2dnT1BQSq82NDSlrbRaLcePH9G1+8tKlizFqVMnAdi/fx9/\n/XWGsLBQHBzyYmxszPHjR0hKSk713hdD8S//9z5FHcDY2JhPPinKpUspoylHjhykdu26VKtWEz+/\n4yQkJKDRBBEUFETRoinfUAgKCsTe3uG9jvNfGFSPXRiWO3f+wdl5JCdOHANSPm1HRUViaWmVzjuF\n+O+cncfh7j4WgKZNm712Dbl//0HMmOHJjh2/YmxswvjxE1/bR/v2HRk8uB+FCxehW7eerFq1jHr1\nGtKwYWPmzZvNjBlzgJSbz6pWrc533/UBtLRr1/G1fbVo8RXTpk3m0KH9fPttJ/bv/4N9+3bTq1d/\nhg7tT6FCRShTphxqtZqmTZtx/vxZBg3qS1JSEn37Dkz3fIcPd8LNbQzVq9ekYsVKDBw4BGfn4ZiY\nmJCYmEjHjo58+mlVALp27cnatavo27cbuXJZY2lpycyZvqluXLWwsKBfv0GMGjUEgM6du6JSqWjb\ntgMDB/aiTJnSuiHyV31I+1SrVoNLly5SunRZ2rRpz9y5s8mXrwAdOnTG23s6Z86cSvX6kSPH4O09\nnfXr12BqasaUKdNQq41Yv34Nw4YNpEGDRnz2WX3mzJnB+PGT0m2/F4KCAvHwcCc4+DmxsbHcuHEN\nZ+dxBAQ84+nTJ7Rr14ERI5yZPdsLrTaZ8uUrUrNmbQC+/rotQ4cOQKVSMWbMON2lj0uXLtCqVcaN\nOKu0aV0gyGQGLAnGq6NJ+i8UH8ze3oqgoIj0X5iOhIQElixZwJw5M4mLiyNPnjxMmzaL9u07pho2\ny64+VjuLtBlSGx86tJ/q1WuSK5c1Tk7D6NNnAJUqfap0rHR97DZ+/lyDq+toVqxYm6X+Tty6dZNl\nyxYze/b8D3q/vf37d4Skxy4+qqSkJFq3bs6FC+cB6NSpCx4eXuTJIxMLCfEmsbGxjBgxGAsLc0qW\nLGMQRV0f8uSx4+uv27Bx40907dpT6TgfRVJSEt9/v5AxY8Zl6HGlsIuPysjIiObNW/D8eTBz5syj\nceOmSkcSIlNr2bI1LVu2VjpGptC2bdaabdLIyAhf34UZflyDunnOcvJJpSOINzh48E/27t2t2x4x\nwokjR/ykqAshhAIMqrBbbL2ldATxEo1Gw6BB/XB0/JbRo4cSHPwcSPmees6caU/wIIQQQn8MqrCL\nzEGr1bJ58wbq16/B9u1bsbCwYNiw0brpK4UQQihHrrGL93Lv3l1cXEZx5MghABo2bMKcOSkTeAgh\nhFCeQRX2iCl1lY6QrWm1Wvr27cHVq5exsbFh6tQZdOrUJUt9NUUIQ7Ry5VL+/HMvdnb2AMTFxdK9\nex8aNWoCwI0b11myZD4xMTEkJibQoEFjevXqp5u5b+/eXWzduglT05TvvHft2pMmTZopdj6Q8jWx\nH39czvTpsxXNERcXx+zZXty9e4eVK3967fnIyEg8PCYQGRmJhUUOpkyZRq5c1pw9e5plyxajVhtR\nt249evfuz6JF86hcuYpuHQF9MajCHtuptNIRsqUXs02pVCqmTZvJ2rWr8fScib29vdLRhBD/17Gj\nI99+2xmA8PAwevfuSp06dUlKSmLKlAl4es6kVKnSaLVa5s/3YdWqZQwYMJjLly/y889bmDdvCVZW\nVoSEBDNoUF9KlChJkSJFFTufOXNm4OExQ7Hjv7BkyXxKlSrN3bt33vj8li0bqFq1Ol279uS337az\nbt0ahgwZwfz5c/DxWYi9vQPDhg2kUaOmDBgwmIEDe1OrVh3Mzc3fuL+PwaAKu8hY0dHReHt7ERUV\nxezZKWsJf/ZZfT77rL7CyURm4eCQemGQwECnN75u7drLjBmzX7fdo0clfHyav/fx0lu2tUKFiixc\n6MutWzeIiorRLdv57NlTpk2bTHJyMvny5WfChCnMnOmpW1LVw2OGbpnU+Ph4+vcfRK1adV45t9eX\nEj1x4hhRUZH06TMAgOHDv2PkyDE8fvyQTZvWYWRkTJky5Rg+fDS7d+/g1KmTaDRBeHh4sWnTuteW\nF/3nn1tMnz4ZS0srypYtT2hoCBMmTOHnn7ewf/9eVCo1DRo0pkuX7m9tp1y5rMmTxw6NRsPZs6do\n2LARpUqldIxUKhXffTeULl3a07//IH7+eTN9+w7QzcxnY2PLihU/vTZT382bN/DxmYVaraJixU+Z\nMsWdYcMG4uTkSvHiJfn5582EhoZStWp1Nm1aR3R0NNWq1UClUr1T+7zs0qWL2Nraki9fPhITE5k+\nfQpBQYHExMTQt+9A6tVrwLBhA3ULwgwaNAwvLw8iIiJISkpi1CgXSpYsxR9/7GHbts0YGakpWrQE\nY8dOSHWcuXO9uX37n1SPeXnNTnW/0HffDSUsLIw//tj7xrY+d+6sbma7evUa4uo6isePH2FllYu8\nefMBULduPc6dO0OxYo7Uq9eAP//cm2rVuY9Nr4Xdy8uLS5cuoVKpcHNzo3LlyrrnTp06ha+vL2q1\nmmLFijF9+nTd9HtCeUeOHGLMmJHcv38PIyMjhg4dIdfRRabwtmVbS5YsRb58BZg6dTKPHgXplu1c\ntmwJjo7dqF+/EUuWzOfGjevAv0uq7tmzE1NTUxYtWoZGE8SwYd+xadP2VMd901KinTp1wd3dlT59\nBhAeHkZISDAFChTEy2sKP/ywGlNTUyZOHKdbpS0g4Bk//LCK+Pj4Ny4vunr1Mnr3HkCjRk2YOHEc\n5ubmPHnymMOHD7BkyUoABg/uR5MmzciXL1+abfTgwT1CQoJxcMjL/fv3dSvMvWBhYYGtrS0aTdD/\nl3gtk+r5V4s6wLx5c3BxcaNkyVJ4ek7i8ePHaR7/9u1/2LhxOyEhwe/cPpUrV9G9//z5s7rpbyMi\nwqlVqw4tW7bm8eNHTJw4jnr1UhZ3KV68BG3bduDHH1dQu/ZnfP11W+7evcP8+XOYN28JMTEx+Pgs\nxMrKiqFDB3D79j+UKFFSd5zRo13TPIcXcuTImWrRl1c9f/5ct1zri6Vag4Ofv7aE64v2qlKlGnv2\n7DTMwn7mzBnu37/P5s2buX37Nm5ubmzevFn3/KRJk1i7di358uVjxIgRHDt2jEaNGukrjnhHwcHP\nmTx5Aps3bwBSlmKcO3ehFHWRabxt2dYXy3Y6Ojqi1ap0y3bevHmDkSOdARgyZCQAv/66LdUSrlWr\nVgdSFiMxNTUhPDwsVc/tTUuJpvTIVGg0Gv766zQNGjTm7t07BAQ8w8kpZUnYqKhInj17BqQsPKNS\nqdJcXvT+/XtUrpwy81z9+g35668zXL/uz6NHDxk+/DsgZUGXZ8+evFbYt27dxKFDB4iOjiI+PoHJ\nk6dhYmLy/yVcX1+lTavVolarUalSZkhLz4MH9ylZshQAEydOfetUpyVLlsLU1PS92uelfh8aTRDV\nqqUsYWtllYvr1/35/fftqFRqwsP/LbLlylUE4MqVy4SGhrBvX8p8GnFxsUDKB7fx453/37Z3CQsL\nTfc8/4vMsoSr3gq7n58fzZql3HxRokQJwsLCiIyMxNLSEoDt27frfra1tdWtQiSUs3HjRkaMGIFG\no8HMzIwxY8YxZMiI19aAFkJJb1u29cWynevX/0RoaKxu2c60lnF90xKukLLeQXx8gm798K5de3L4\n8IE3LiXasGFjTp48xpkzfvTo0ReVCsqUKYev76JUx9q9e4fueGktL5pyP0vKyOXLS7jWrVsPV9fU\nw8ivenGNXaPRMHLkIEqUSCnCL5Zw/fLLVrrXRkdHEx4eTp48dhQpUpTr1/11w8aQ8gHD3t6BHDly\n6B5704jqyzfOvmkJ1/dpn7T2/eefewkPD2fx4hWEh4fTv3+Pl45jrPvf0aNdqFjx308HCQkJ+Pp6\n8+OPG8iTxw5X11GvHeNdhuLTY2dnR3CwBktLy1RLuL6Y1wMyfglXvY19azQabGz+HYqwtbUlKChI\nt/2iqAcGBnLixAnpf0lCsgAAIABJREFUrWcC+/btQ6PRUK9eAw4fPsnIkc5S1MVbBQY6pfovLT17\nVk71ug+5vv4uXizbaWJikmrZzrJly3P+/FkAVqz4gbNnT6d638tLuAYEPEOtVmNnZ6db2vOzz+qn\nuZRoo0ZN8PM7waNHjyhTpixFihTl3r27hIQEAyl3rAcFBb4x56vLixYsWIgbN64B6JYkLVOmHOfP\nnyM2NhatVsu8eXN0PdI3sbOzo0WLr1i9ejkAX3zRkpMnj+n2C7Bs2RLdUPC/S7ym5H3+XMPEiWMJ\nCHiWar9FixbD3/8qADNmTOX27dvkzJmT5881AFy5cumNeT6kfV5dwjV//gKo1WqOHDn4xiVcy5ev\nyNGjhwG4e/fO/6/xR2FkZESePHYEBDzjxo3rqT58QMpQ/KvLuL7vfBy1atXh4MGU+0dSPvzVJX/+\nAkRFRfH06RMSExM5efI4NWum3LOh0eh/CdcMu3nuTUMUz58/Z9CgQUyePDnVh4C02JdfA0HD9BEv\nW0pKSuLp06cUKpSyRrSPjw+NGzemV69e8hU2PfuQFZsEWFmZkyOHKfb2VlhbW2BubpLq5xYtPmfz\n5nV0796dZs2a0aRJYxYtmoOrqzPjx49n585fyJ8/P19+2YSjR/djbW2Bvb0Vjo7fcv36ZZychpCQ\nkMD06dNe+x317NmNWbNmUbBgQXr06MHEiRP5++9L1K9fn8DA/7V33wFV1f8fx5+X6QAFFRWcOXKW\nExVBTRO3lmmMxIWBKA7cqBgoKuROcuUelZpfszRX+tNy4DZniYEpIE7URAG5cH9/3LxBXIbG5XJv\n78dfes+597x5l7zvOffczyuBdu3a/PUca6ZNCyIwcAwWFhbUr1+fevVq8NtvFzS1v6xzzJhhWeoc\nPXokQUFB7NixlVq1avH06VPeeqs23t6DGD16KKampnTs2JHKlbN+I6VkSUusrIppah4xwo+ePXvS\nr587tWvXZvXqVYSEhPDs2TOUSiUuLi4EBIzEzMyM9u1bo1KNZ+LE0RQvXhwzMzOCgz+hRYusYTQh\nIZ8QEhICQOPGjalZsyb9+/cjLCyMatWqUbVqVUqWtMTGpgSWluaaWuzs3spXfzL/zunQoS3r1q3D\nzs6aDz7oybBhw7h+/Vf69OmDg4M9W7asx8LCDFvbktjZWTN06BAmT57M6NFDycjIYOrUqdSqVYU2\nbVzw8xtE3bp18fX1YenSRXTqtOOVTlhGjRrFnTt3iI29ydixw3Fzc6NVq1ZEREQwY8YM/Pw+ZsKE\nCYwePZRSpUoxd+5crK2tmTlzBrNmqW+q69WrB82aqT82iIq6Qrt2Ljr9HaCz2NaIiAjs7Ozw8PAA\n4N133+W7777TnKknJSUxYMAAAgICaNu2bZ6v57M0kZXBX3H/6kBdlPufc/nyJcaNG8mzZ884ePAo\nlpaWBhV1acikz7pnqD2+fPkSxYoVo1at2mzcuBaVSsWAAd76LksrXffY13cQoaHhWT4iMHSpqan4\n+g5i+fI1FC9ePF/PeZ03ADq7FO/s7My+ffsAuHLlCuXLl9cMdYDw8HAGDhyYr6EuCk5ycjIzZ4bQ\nqVM7zp8/R1JSEjdv/qHnqoQQABYW5oSHh+Lv78P58+d4//0++i5JbyZMmExExIK8dzQgq1Ytx9vb\nJ99D/XXp7IwdYN68eZw5cwaFQkFwcDBXr17F2toaFxcXHB0dadKkiWbfHj164O7unuNryRn7v3f0\n6M+MGzeKGzdiUCgUeHv7MHWq+juzYLhnOYZG+qx70mPdkx4Xjtc5Y9fpZ+zjx4/P8ve6detq/nz5\n8uVXfj0Z6q8vJCSIpUsXA1C3bj3mz1+Mo2NLPVclhBCioMnKc/8RdevWw8LCgjFjJjBypPqmFSGE\nEMZHBruRio+P49y5s/Ts+R4A7u4f4eTkTLVq1fVbmBBCCJ2SNVyNTHp6OqtXr8DFpQXDh39MdPR1\nQL3Ygwx1IYQwfnLGbkR+/fUqY8eO5OxZ9UIc3bv30twYJ4QxSEi4zYABHtSpo75fJy0tjRo1ajF+\nfCCmpqakpKQQEbGAqKhfAQW2tmUZN26S5itTsbG3WLx4Po8fPyI9PYO33nobf/+AIvfR1LlzZ5gz\nZxa+vv506JBzfGpCwm2CgiZpjRN9XTnFkGaWnp6Oj48P/v5jqVSpcoEd+1Xlp9bVq1dw4sRxzMxM\n8fMbRaNGjXn48AGzZk0nNTUFW1tbpkwJoUSJEhw5cpj169dgbm5Ox46d6NPHvdCiVguSnLEbgZSU\nFMLDZ9KxYxvOnj1NhQoVWbv2S9au3WRU3wEVAqBq1WqaVcJWrFiLUpnGjz+qk7ciIhZQrpwdO3bs\nYOXKDXh5DWTcuFEolUrS09MJCprIRx8NYOXKDZph+HKFtqLkwoXzfPDBh7kOdV15GUO6bNlq2rVr\nz6ZN67Pts2PHNpo3b67XoQ551xoV9RunT59kxYq1fPrpIpYvV99AvHHjOtq0aceSJStxcWnHtm2b\nycjIYOHCucyd+xlLlqzk2LEj3Lt3Fx+fYaxevYKUlJxX+ytqDOqMvdjWKMlk12LSpLF8/fUmAAYO\nHMK0aSGvvCyiEIaqfv2GxMXF8vz5M06cOM6WLTs0295+uzH16zfgyJHDFC9egqpVq2vCXhQKBcOH\nj9Kszf6SUqlk5sxg7t5NwMLCkqCg6Zw+fZKYmGhGjAjg+fPnDBjgzrZtO/Hw6E2rVs7Y2tqyZ88P\nmkS4PXt28fvvUXh69icsLBSlMg0TExMmTZqWLbxl6dLPuHTpAkplOn36uFG7dh1++OF7zMzMKFu2\nHO++20mz75dfrufw4YMoFCb4+Y3A3t5Bs01bROmdO3cIDZ2GiYkJ6enpfPJJKKDI9ljFivaa19EW\nQ/pP27ZtYdu2b0hL037cf0bU/vzz4Wyxs9picDO/UTh+/ChffbUhy3F79fqATp265LvW2NhY6tSp\ni4mJCaVKlaJkSSsSEm4TF3eLLl26A9CypRPTpgXSs+f7WFlZaVZBbdbMkTNnTtGtW89CiVotSAY1\n2K1DImWwazFq1BguXbrI7NlzadXKSd/liP+Ij0oV54Blwf4K6Ziq5Ks/k/O9v1Kp5MiRn3j//T7E\nx8dRrVp1zMyy1lS7dh1u3bpJ8eLFNZnkL1laFsv2mnv27KJs2bKEhMziwIF9HD36M5aWljkev1Wr\n1rRq1Zpz584QExNNjRo1OXLkJzw9vVi5chkeHv1wdGxJZORR1q9fxaRJQZrn//LLOWJiolm2bA3J\nyckMHOjBunVf0bVrD2xsbLIM9djYWxw+fJAVK9Zx+3Y8mzatY+DAIZrt2iJKT58+gaNjSwYN+phr\n137jwYMHXL58IdtjmQe7thjSzO7cuYOFhQU2Njbcv/9U63Hh74jahITbWmNnHz16mC0GN3Mue+vW\nLrRu7aK17/mttUaNmmzYsJqUlBSeP3/G9etRJCYmUqNGLSIjj1K3bj1OnDjO48ePsLGx5fnz58TG\n3sLe3oFz587SpElToHCiVguSQQ12obZr1/fs3fsDERHLUSgU1KxZm4MHj8j67uI/4datm5rUtejo\n3+nXbwBt277D9etRpKfnFE9qCii0xpf+07Vrv9G8uToytGPHzoA6mS0nL6Nf27Ztz7FjR6hUqTI3\nbkTTsOHbhIeHcuvWTdavX01GRkaWjG6A3367SuPG6uFRvHhxqlevQWxsrNbjREVdo379hpiYmFC5\nchUCA6eRkHBbs11bRGmLFq2YMmUCT58+pX37d2nY8G1KlCie7bGcaFu/7MGD+1lCTHKKRn0ZUZtT\n7Ky9vUO2GNx/Q1utb7xRg169ehMQMBwHh0rUqvUmKpWK/v0HM29eGCNG+OLk5PxXqp6CqVNDCAub\ngZWVFfb2Dpq41cKIWi1IMtgNyJ07CQQGjtf8kunR4z26dFFHMcpQF4XtVc6sC9LLz9gBgoImUqVK\nNQAqVapEbOzNbOlfv/8eRdu272BubsH//rc1y7YXL14QF3eLGjVqaR4zNc0e8ZpTPCn8Hf3arl17\npk0LpEaNmrRs6YRCocDMzJzQ0E9zjOxUKBRZsrrVl+y1/1vWVtdLOUWU1qhRi3XrvubUqRMsX/45\n3bv3omvXHlofe0lbDKm2unM7bua+5BQ7O3v2dK0xuC/l51J8fmrt08edPn3Uq5oOHToYe3t7rK2t\nmT59NgC3bv3B2bPqZL8mTZqxdOkqAJYv/xx7e/tsr2cIDOrmueQPa+u7BL3IyMhg3brVODs7snv3\nTkqWtCIsbC6urp31XZoQejV8+GiWL48gJSWFEiVK0rp1G9as+UKz/dKlC0RFXcPJyQVHx5bcvZvA\n0aM/A+p/V8uWRXDw4I9ZXjNzxOuxY0fYsGENJUr8HU968eIvWmspV84OhULBgQP7eOeddwH15/9H\njhwG1J8H79+/9x/HasD582cBdUZ6fHwclStX1fr6derU++uzeCWJiQ+ZPPnvlT1ziig9cGAfMTG/\n07btO/j4DOfatV+1PpaZthjSf/6c9+7dy/W4/6xbW+xsTjG4L7Vu7ZItUjXzUM9PrY8ePWL8+FGo\nVCpiYqLJyMigbNlyfP/9t+zYsQ2AH37YibNzGwDGjRvFo0eJJCcnc+zYzzRvrl6dszCiVguSQZ2x\nJ01vre8SCt3161GMHTuSkycjAejcuSvh4fP1fjeqEEWBg0Ml3nnnXdavX83Qof6MHj2O5csj6NWr\nFyYmptjY2BIaGo6pqSkA8+d/zpw5s1i7diXm5uY4OrZk8GCfLK/ZsWNnzpw5xYgRvpiamhEUpP4q\n1IYNaxgxwpfWrV2y3XD3kotLW775ZjPTps0AYMgQX2bPns6BA/tQKBRMmRKcZf9GjRpTp05d/P19\nUCqV+PmNyDEgxN7egc6duzFihC8qlYqhQ/0120qXtsHRsSUffzyAWrVq89FH/Vm8eAGTJ3/CwoVz\nKF68BCYmJgQETCA1NZV582ZneSyzvn09CA2dxvDhH2NlZf3XDXd/q1ixIqmpqTx58iTH47q5eWbZ\n383NE39/H0xMTGjb9h0sLYvx3nsfsHDhXCpWdKBvX3fmzJnFqVMnaNGiVW7/yfNV62efzefDDz1w\ncKhE7dp1GDKkP6amJkycqL6/oU2bdgQFTWL37l1UqlQZH59hAPTq9T5jxoxAoYD+/QdjY2MDwC+/\nnKdp0+b5rkvfdBoCU5B8liYy+8P8Z+gai7lzw5g7Nww7u/KEhc2lZ8/3dXbZXUIdCof0Wfekx7r1\nzTebMTNT0bu3Z947G7jXiVotSEUqtlW8vj//fKL586hRYxk7dgLHjp2mV6/e8lm6EELvevfuy+nT\np4mPj9N3KTpXWFGrBUnO2IuQp0//ZObMEHbt+p4jR05SpkzZQj2+nOUUDumz7kmPdU96XDjkjN2A\n7d27GxeXFqxdu4pHjxI5fvyYvksSQghhgAzq5jljdPfuXaZMmcDOnerVspo0acqCBZ/ToEFDPVcm\nhBDCEBnUGbtN35wXiTBEO3d+h4uLIzt37qBEiZKEhoaxe/dBGepCCCFem0GdsZtfTdR3CQWqTJky\nPHnymHffdWXOnIVUqaL9+6tCCCFEfhnUYDd0aWlp/PzzIc36z87Obdiz5yBNmzaXu92FyAeJbc1K\nF7GtAP/3fwcIC5vOihVrs6zK95LEthZtBnUp3pCdO3cGV9d2eHr2zbJ0YrNmjjLUhXgFEtuqW+fP\nn+XEiWPUrJnzSp8S21q0GdQZ+6NveuS9UxGTlJREeHgoq1atICMjg2rVqmNiIu+nhCgoEtuqVlCx\nrXXq1KVJk2aaoB1tJLa1aDOowa5sULjf6/63DhzYx8SJY4mLi8XU1BR//9FMmDCZEiVK6Ls0If61\nUgf7Yhm/v0BfM7VSJ/58d1u+95fY1oKPbS1RomSuPZfY1qLPoAa7IVm/fg0TJqjfPb79dmMWLozg\nrbca6bkqIQyfxLYWXmyrNhLbWvTJYNeRnj3fY/HiBQwZMhRf32HZziKEMHSvcmZdkCS2NbuCjG3N\nD4ltLdrkw94CcuNGDOPGjSY1NRWAMmXKEhl5juHDR8pQF0JHJLaVv55bcLGteZHY1qJPJs6/pFQq\nWb58CXPnziY5OZmqVasyerT6slRR+wqNEMZGYlvVCjK2ddeuHezdu5vff49i9uwZVKtWXfPzgMS2\nGgKDCoGZU//PInUD3cWLvzBmzEguXboAQJ8+boSGhud42a2ok1CHwiF91j3psW5JbGvhMfoQGNsP\nd+m7BEB9ySwkJIhOnd7h0qULVKlSlc2b/8eyZasMdqgLIUR+SWxr0SaX4l/D3r0/sHTpYkxMTBg6\n1J9Jk6ZiZWWl77KEEKJQmJmZsXLlyv/EVRF//9H6LuGVyWDPp7S0NMzN1Xd5qt+tnsTNzVOz2IUQ\nQghRFBjUpfi0+mUK/ZgqlYpt27bQokUjYmLUCy8oFArCwubJUBdCCFHkGNRgf7ytZ6Ee79atm3h4\nfMDw4T7Ex8exadOGvJ8khBBC6JFcitciPT2dlSuXER4+k+fPn2NjY8P06bPx8Oin79KEEEKIXMlg\n/4eoqGuMGOHLL7+cB+D99z9g5sw5lC9vOIsTCGGsJLY1K13EtiYlJTFz5ickJSWRkZHBxIlTqV79\njSz7FJXYVsg7YvarrzZw6NABQIG3tw9OTi45xr2ePn2SL75YgomJKU5Ozgwa9LHEthoDExMTfv31\nKg4Oldi0aQtffLFOhroQRYjEturWli1f8tZbjfj88y/w8hrE6tUrsu1TVGJb84qYvX07ngMH9rN0\n6WrmzFlERMRC0tPTc4x7/eyzecycOYdly1Zz6tQJbtyIkdhWQ3XhwnnefrsxCoWCWrVqs37917Ro\n0RIrq1dfGEAIUbgktlWtoGJbvbwGaaKlbWxs+PPPJ9l6XlRiW/OKmD137gytWrXG3NwcW1tbKla0\n548/bmiNe42Pj8PaupTm6o6TkzNnz57ijTc8JLbVkDx6lEhISBBff72Jzz9foVkGUR/vkoUwNOuP\nKLl2p2AXrqxTUcHANvn/tSSxrQUf25r5Z/3mm824unbO8jMXpdjWvCJmExMfZknUexntqi3uVdu+\n8fHxgMS26pRV8HGSprf+16+jUqn47rvtTJkykQcP7mNhYcHjx48KoEIhhK5JbGvhxLYuXboYc3Nz\nevTIOsyKamxrfmhbQD2nVdUzPyyxrTpU/Jvr/3qwx8XFMmnSWH78cR+gvtwyf/5iatXS/hmNEEK7\nVzmzLkgS25pdQce2rlq1nMePHxEYOC3HunM7bua+6DK2NS/lytlx69ZNzd/v379HuXLltMa9litn\nR2Liw2z7GqL/1M1zZ8+epk2blvz44z5KlSrNvHmf8e23P8hQF8JASWwrfz234GJbL1z4hatXrxAY\nOE3zWfs/f86iEtual6ZNHYmMPEpaWhoPHtzn/v37VK9eQ2vcq729A8+ePSMh4TZKpZLjx4/i6KhO\nmpPY1iKsYcO3cXBwoE6deoSFzdXcJCGEMEwS26pWkLGt3377Dffu3WHUKD8ASpUqzezZczXbi1Js\na04Rs5ljW3v2fB9/fx8UCgXjxwdiYmKSY9zr+PGBhISoryx06OBK1arqq0ES26ojPksTWaC6QYrb\nm3nv/JeUlBSWLYtg0KAh2Nqql6N9/PhRts+5hJpEXRYO6bPuSY91S2JbC4/Rx7a+ylCPjDxGhw7O\nhIWFEhLy912oMtSFEOLfkdjWos3oLsU/efKYGTOC2bhxLQC1a7+Jp2d/PVclhBDGQ2JbizajGuy7\ndn3P5MnjuXv3Dubm5owaNZaAgPE5fgdVCCGEMDZGM9ivXLmMt7cXAM2bt2DBggjq1tX99yKFEEKI\nosSgB7tKpdJ8n7JBg4YMHTqcN96oyaBBQ7R+TUMIIYQwdgY7/aKirvHee12JjDymeSw0NBxvbx8Z\n6kIIIf6zdDoBZ8+ejbu7Ox4eHly8eDHLtuPHj9O3b1/c3d1ZsmRJvl7Prv56UlNTmTs3jA4dnDlx\n4jiffjpLF6ULIYQQBklnl+JPnTrFzZs32bJlC9HR0UyZMoUtW7Zots+cOZPVq1dToUIFvLy86Ny5\nM7VqZc/Szex4WgzeHdtw7dpvAHh5DeSTT2bo6kcQQgghDI7OBntkZCQdO6pT0mrWrMmTJ09ISkrC\nysqK2NhYSpcujb29OlGoXbt2REZG5jrYj2+ZyOona1A9UVGjRk0WLIjIM/lHCCGE+K/R2aX4Bw8e\nYGv792IwZcqU4f79+wDcv3+fMmXKaN2Wk5hzOzBFQUDAeA4fjpShLoQQQmhRaHfF/9uVa5OfPiig\nSkRuXmf5QvHqpM+6Jz3WPelx0aSzM/by5cvz4MHfw/jevXvY2dlp3Xb37l3Klzec5BwhhBCiqNLZ\nYHd2dmbfPnXm+ZUrVyhfvjxWVlYAVK5cmaSkJOLi4lAqlRw6dAhnZ2ddlSKEEEL8Z+g03W3evHmc\nOXMGhUJBcHAwV69exdraGldXV06fPs28efMA6NSpE0OGDNFVGUIIIcR/hsHEtgohhBAib7JEmxBC\nCGFEZLALIYQQRqRIDvaCXopWZJdbj0+cOIGbmxseHh5MnjyZjIwMPVVp2HLr8Uvz58+nf//+hVyZ\n8citxwkJCXh6etK3b18++eQTPVVoHHLr85dffom7uzuenp7MmiVLfL+uqKgoOnbsyKZNm7Jte+W5\npypiTp48qfL19VWpVCrV77//rnJzc8uyvWvXrqrbt2+r0tPTVZ6enqrr16/ro0yDllePXV1dVQkJ\nCSqVSqUaOXKk6vDhw4Veo6HLq8cqlUp1/fp1lbu7u8rLy6uwyzMKefV41KhRqv3796tUKpUqJCRE\nFR8fX+g1GoPc+vz06VNV+/btVWlpaSqVSqUaPHiw6vz583qp05A9e/ZM5eXlpQoKClJt3Lgx2/ZX\nnXtF7ow9p6VogSxL0ZqYmGiWohWvJrceA2zfvp2KFSsC6lUBHz16pJc6DVlePQYIDw9nzJgx+ijP\nKOTW44yMDM6ePUuHDh0ACA4OxsHBQW+1GrLc+mxubo65uTnPnz9HqVSSnJxM6dKl9VmuQbKwsGDl\nypVa13N5nblX5AZ7QS9FK7LLrceAZr2Be/fucezYMdq1a1foNRq6vHq8fft2WrRoQaVKlfRRnlHI\nrceJiYmULFmSsLAwPD09mT9/vr7KNHi59dnS0hJ/f386duxI+/btadSoEW+88Ya+SjVYZmZmFCtW\nTOu215l7RW6w/5NKvo2nc9p6/PDhQ/z8/AgODs7yj1q8nsw9fvz4Mdu3b2fw4MF6rMj4ZO6xSqXi\n7t27DBgwgE2bNnH16lUOHz6sv+KMSOY+JyUlsWLFCvbu3cvBgwe5cOECv/32mx6rE1AEB7ssRat7\nufUY1P9YfXx8CAgIwMVFwnZeR249PnHiBImJifTr148RI0Zw5coVZs+era9SDVZuPba1tcXBwYGq\nVatiamqKk5MT169f11epBi23PkdHR1OlShXKlCmDhYUFzZs35/Lly/oq1Si9ztwrcoNdlqLVvdx6\nDOrPfgcOHEjbtm31VaLBy63HXbp0Yffu3WzdupXPP/+cBg0aMGXKFH2Wa5By67GZmRlVqlThjz/+\n0GyXS8SvJ7c+V6pUiejoaFJSUgC4fPky1atX11epRul15l6RXHlOlqLVvZx67OLigqOjI02aNNHs\n26NHD9zd3fVYrWHK7f/jl+Li4pg8eTIbN27UY6WGK7ce37x5k8DAQFQqFW+++SYhISGYmBS5cxmD\nkFufN2/ezPbt2zE1NaVJkyZMnDhR3+UanMuXL/Ppp58SHx+PmZkZFSpUoEOHDlSuXPm15l6RHOxC\nCCGEeD3y9lUIIYQwIjLYhRBCCCMig10IIYQwIjLYhRBCCCMig10IIYQwImb6LkCI/4K4uDi6dOmS\n5WuEAFOmTKFevXpanxMREYFSqfxX68mfPHmS4cOHU79+fQBSU1OpX78+U6dOxdzc/JVe6+eff+bK\nlSsMGzaMc+fOYWdnR5UqVZg1axbvvfceDRs2fO06IyIi2L59O5UrVwZAqVRSsWJFZsyYgbW1dY7P\nu3v3LjExMTg5Ob32sYUwNjLYhSgkZcqU0cv31d98803NcVUqFWPGjGHLli14eXm90uu0bdtWs2jR\n9u3b6datG1WqVGHq1KkFUmevXr2yvImZO3cuy5cvZ8KECTk+5+TJk0RHR8tgFyITGexC6Fl0dDTB\nwcGYmpqSlJREQEAAbdq00WxXKpUEBQVx48YNFAoF9erVIzg4mBcvXjBjxgxu3rzJs2fP6NGjB97e\n3rkeS6FQ0KxZM2JiYgA4fPgwS5YsoVixYhQvXpzQ0FAqVKjAvHnzOHHiBBYWFlSoUIFPP/2UXbt2\ncfz4cTp37szevXu5ePEikydPZunSpQwbNoz58+czdepUmjZtCsCgQYMYPHgwtWvXZvr06SQnJ/P8\n+XPGjh1L69at8+xLkyZN2Lp1KwBnzpxh3rx5WFhYkJKSQnBwMKVKlWLRokWoVCpsbGzo16/fK/dD\nCGMkg10IPXvw4AGjR4/G0dGR8+fPExoammWwR0VFceHCBfbs2QPA1q1befr0KVu2bKF8+fLMnDmT\n9PR03NzcaN26NXXr1s3xWKmpqRw6dIi+ffuSnJxMUFAQ27Zto2LFimzatIlFixYRGBjIl19+yZkz\nZzA1NWX37t1Z1qp2dXVlw4YNDBs2DCcnJ5YuXQpAz5492bdvH02bNuXhw4dER0fj4uLCsGHD8Pb2\nplWrVty/fx93d3f279+PmVnOv36USiW7du2icePGgDo4JyQkhLp167Jr1y5WrFjB4sWL6d27N0ql\nksGDB7Nq1apX7ocQxkgGuxCFJDExkf79+2d57LPPPsPOzo45c+awcOFC0tLSePz4cZZ9atasia2t\nLT4+PrRv356uXbtibW3NyZMnuXPnDqdPnwbgxYsX3Lp1K9sgi4qKynLc9u3b061bN3799VfKli1L\nxYoVAWjRogX4CuQlAAAC1UlEQVSbN2+mdOnStGnTBi8vL1xdXenWrZtmn9x0794dT09PJk+ezN69\ne+nSpQumpqacPHmSZ8+esWTJEkC9jvvDhw+pUKFClud///33nDt3DpVKxdWrVxkwYAC+vr4AlCtX\njjlz5pCamsrTp0+1Zn7ntx9CGDsZ7EIUkpw+Yx83bhzdu3enb9++REVF4efnl2W7paUlX331FVeu\nXNGcbX/99ddYWFjg7+9Ply5dcj1u5s/YM1MoFFn+rlKpNI8tXryY6OhofvrpJ7y8vIiIiMjz53t5\nM93FixfZs2cPgYGBAFhYWBAREZElU1qbzJ+x+/n5UalSJc1Z/cSJE5k+fTpOTk4cOnSINWvWZHt+\nfvshhLGTr7sJoWcPHjygdu3aAOzevZsXL15k2X7p0iW+/fZbGjRowIgRI2jQoAF//PEHzZo101ye\nz8jIICwsLNvZfm6qV6/Ow4cPuX37NgCRkZE0atSI2NhY1q1bR82aNfH29sbV1TVbxrZCoSAtLS3b\na/bs2ZNt27bx5MkTzV3ymetMTExk1qxZedYWHBxMREQEd+7cydKj9PR09u7dq+mRQqFAqVRmO87r\n9EMIYyGDXQg98/b2ZuLEiQwZMoRmzZpRunRpwsPDNdurVq3Kvn378PDwYMCAAZQqVYqmTZvSr18/\nSpQogbu7O25ublhbW2NjY5Pv4xYrVoxZs2YxZswY+vfvT2RkJAEBAVSoUIGrV6/St29fBg4cSHx8\nPJ06dcryXGdnZ4KDg9m/f3+Wxzt16sTOnTvp3r275rGpU6dy4MABPvroI3x9fWnVqlWetdnb2+Pj\n48O0adMA8PHxYeDAgfj5+dG7d28SEhJYt24dzZs3Z/v27SxatOhf90MIYyHpbkIIIYQRkTN2IYQQ\nwojIYBdCCCGMiAx2IYQQwojIYBdCCCGMiAx2IYQQwojIYBdCCCGMiAx2IYQQwojIYBdCCCGMyP8D\nzowlcvcz/WcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kSbfQIVmzD5P",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from string import ascii_uppercase\n",
        "import seaborn as sn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "72291a7c-53f0-4182-b950-9384b72b1ebf",
        "id": "tPXPJseqzD5V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "# Make a confusion matrix at first\n",
        "matrix = confusion_matrix(y_test.argmax(axis=1), pred.argmax(axis=1))\n",
        "\n",
        "# Convert the matrix into a data frame as input for the heatmap\n",
        "col1 = ['Real: Austen', 'Dumas', 'Nabokov']\n",
        "col2 = ['Austen', 'Dumas', 'Pred: Nabokov']\n",
        "df_cm = pd.DataFrame(matrix, index=col2, columns=col1)\n",
        "ax = sn.heatmap(df_cm, cmap='Greens', annot=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAFKCAYAAABlzOTzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bvUsLAzabQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/fy164251/text_style_transfer/master/outputs/all_scores.csv'\n",
        "df = pd.read_csv(url).set_index('Unnamed: 0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoYs6lLFID3R",
        "colab_type": "code",
        "outputId": "f842f7bd-16e7-4e25-ebf1-dd226a7f77c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "\n",
        "x = df['donor_naturalness_score'].values\n",
        "y = df['style_naturalness_score'].values\n",
        "\n",
        "u = df['naturalness_delta'].values\n",
        "v = df['authorship_delta'].values\n",
        "\n",
        "#plt.axis('equal')\n",
        "plt.quiver(x,u,y,v)\n",
        "plt.xlabel('Naturalness')\n",
        "plt.ylabel('Style Shift')\n",
        "plt.xlim(left=0.89, right=1.01)\n",
        "plt.ylim(top=0.04, bottom=0)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEQCAYAAAB80zltAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0us2zadbdKG",
        "colab_type": "code",
        "outputId": "65236382-8f96-4f6b-a66d-be6ae2c5972a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "x = df['donor_naturalness_score'].values\n",
        "y = df['donor_authorship_score'].values\n",
        "\n",
        "u = - df['naturalness_delta'].values\n",
        "v = df['authorship_delta'].values\n",
        "\n",
        "Arrows = plt.figure()\n",
        "for i, j, k, l in zip(x, y, u, v): \n",
        "  plt.arrow(i, j, k, l, head_width=0.02, head_length=0.04)\n",
        "plt.xlabel('Naturalness Score')\n",
        "plt.ylabel('Style Score')\n",
        "plt.xlim(left=0.1, right=1.05)\n",
        "plt.ylim(top=1, bottom=-0.1)\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "plt.savefig('Arrows.pdf')\n",
        "files.download('Arrows.pdf')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEQCAYAAABFtIg2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8yKRNf6C5eK",
        "colab_type": "code",
        "outputId": "a951cf2d-df1f-4ef5-e249-fac5eae68fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import files\n",
        "plt.savefig(\"abc.png\")\n",
        "files.download(\"abc.png\") \n",
        "\n",
        "plt.savefig(('./gdrive/My Drive/DL/Style/arrows.pdf'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGlExpM3wJAF",
        "colab_type": "text"
      },
      "source": [
        "### Appendix: 7 Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP-xHdpvHhBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt = [\"She sat at the window of the train, her head thrown back, one leg stretched across to the empty seat before her. The window frame trembled with the speed of the motion, the pane hung over empty darkness, and dots of light slashed across the glass as luminous streaks, once in a while. Her leg, sculptured by the tight sheen of the stocking, its long line runningstraight, over an arched instep, to the tip of a foot in a high-heeled pump, had a feminine elegance that seemed out of place in the dusty train car and oddly incongruous with the rest of her. She wore a battered camel's hair coat that had been expensive, wrapped shapelessly about her slender, nervous body.  The coat collar was raised to the slanting brim of her hat. A sweep of brown hair fell back, almost touching the line of her shoulders. Her face was made of angular planes, the shape of her mouth clear-cut, a sensual mouth held closed with inflexible precision. She kept her hands in the coat pockets, her posture taut, as if she resented immobility, and unfeminine, as if she were unconscious of her own body and that it was a woman's body. She sat listening to the music. It was a symphony of triumph. The notes flowed up, they spoke of rising and they were the rising itself, they were the essence and the form of upward motion, they seemed to embody every human act and thought that had ascent as its motive. It was a sunburst of sound, breaking out of hiding and spreading open. It had the freedom of release and the tension of purpose. It swept space clean, and left nothing but the joy of an unobstructed effort. Only a faint echo within the sounds spoke of that from which the music had escaped, but spoke in laughing astonishment at the discovery that there was no ugliness or pain, and there never had had to be. It was the song of an immense deliverance.\", \n",
        "       \"She sat at the window of the train, her head thrown back, one leg stretched across to the empty seat before her. The window frame trembled with the speed of the motion, the blackness of space, and dots of light slashed across the glass as luminous streaks, once in a while. Her leg, sculptured by the tight sheen of the stocking, its long line running straight, over an arched instep, to the tip of a foot in a high-heeled pump, had a feminine elegance that seemed out of place in the dusty train car and oddly relatable in the low-slung ranks of her. She wore a battered camel's hair coat that had been expensive, wrapped shapelessly about her slender, nervous body. The coat was clean-cut too, worn into the sleek collar that clung to her neck. A sweep of brown hair fell back, down, down, over her shoulders. Her forehead, above the sun-dappled asphalt, was smudgy and plump, and the wing of her nose, a glossy dark-orange, was exactly shaped as her lips, small stern Plebeian nose thrust below, with downward curved mouth. Her long legs were free, her pink mane kept catching the rays, and the muscles of her hands continued to embrace, as if she suddenly slipped into a woman's body. The silence got broken. It was a symphony of triumph. The notes flowed up, they spoke of rising and they were the rising itself, they were the essence and the form of upward motion, they seemed to embody every human act and thought that had ascent as its motive. It was a sunburst of sound, breaking out of hiding and spreading open. It had the freedom of release and the tension of purpose.\",\n",
        "       \"She sat at the window of the train for half a minute, and as she looked up, she stretched one leg across to the empty seat in front of her. The window frame trembled with the speed of the motion, the unnerveness of the table amidst the scattered glasses, and the distressing tones of the glass pressed against the glass as luminous streaks obtruded the view. Her leg, sculptured by the tight sheen of the stocking, its long line running straight, from the ankle to the fore-foot, to the tip of a foot in a glide, had a feminine elegance that seemed out of place in the dusty train car and oddly incongruous with the rest of her. She was a scantily shaven woman: her hair was very short but expensively cut, her face was a little hoary and attached to a rather pensive, nervous body. The coat collar was raised to the slanting yellow humbug of her hat. A sweep of brown hair fell back, affording a quick view of her face. Her hair was striking, as if unsubdued by ill-timed gravity, and almost turned into a veil against her face, her sensual mouth held closed with inflexible precision. She put one hand tied behind her face, and her heart leaped against the stomach with sudden feeling, as the cottony, trembling hand was about to embrace body her woman's body. She was in a reverie, as he entered the room. It was a symphony of triumph. The Tilneys were at the instrument; they determined, boldly among themselves they were the rising stars, they were the lights of the chambers, where the utmost importance was the form of upward motion, the perfection they encumbered, to examine and express themselves in all their grandeur as if they had been conscious of it. It was a sunburst of sound, breaking out of hiding and spreading over the whole room. It had the freedom of release and the tension of purpose. It swept space clean. There was a feeling from which the music had escaped, and spoke in laughing astonishment at the discovery that there was no ugliness or pain, and there never had had to be. It was the sound of a great pianoforte.\",\n",
        "       \"Elizabeth listened in silence, but was not convinced; their behaviour at the assembly had not been calculated to please in general; and with more quickness of observation and less pliancy of temper than her sister, and with a judgement too unassailed by any attention to herself, she was very little disposed to approve them. They were in fact very fine ladies; not deficient in good humour when they were pleased, nor in the power of making themselves agreeable when they chose it, but proud and conceited. They were rather handsome, had been educated in one of the first private seminaries in town, had a fortune of twenty thousand pounds, were in the habit of spending more than they ought, and of associating with people of rank, and were therefore in every respect entitled to think well of themselves, and meanly of others. They were of a respectable family in the north of England; a circumstance more deeply impressed on their memories than that their brother's fortune and their own had been acquired by trade.\",\n",
        "       \"Elizabeth listened in silence, but then understood that this was a deliberate lie; their behaviour at the assembly had not been calculated to please in general; and with more quickness of observation and less pliancy of temper than her sister, she discerned in her a deceptive, though quite reasonable, form of reflection, and not only because she was concealing her ineptitude to keep up with the jol. They were in fact very fine ladies; not deficient in good humour when they were pleased, but parched; not easily swayed by carnal particularities; not indifferent minds when it came to but proud and conceited. Their meeting with everybody was very warm and well-wishing; and their smiling faces, had been educated in one of the first private seminaries in town, had a fortune of twenty thousand pounds, were in the habit of spending more than they ought, and of associating with people of rank; and were therefore in every respect entitled to think well of themselves, and meanly of others. The greater part of their city consisted of solidly cottoned Estotians; a circumstance more deeply impressed on their memories than that their brother's fortune and their own had been acquired by trade.\",\n",
        "       \"In the meanwhile, Monsieur continued his route with an air at once so melancholy and so majestic, that he certainly would have attracted the attention of spectators, if spectators there had been; but the good citizens of Blois could not pardon Monsieur for having chosen their gay city for an abode in which to indulge melancholy at his ease, and as often as they caught a glimpse of the illustrious ennuye, they stole away gaping, or drew back their heads into the interior of their dwellings, to escape the soporific influence of that long pale face, of those watery eyes, and that languid address; so that the worthy prince was almost certain to find the streets deserted whenever he chanced to pass through them.\", \n",
        "       \"In the meanwhile, Monsieur continued his route with an air at once ardent and melancholy, and I suppose it gave him a thrill of recognition that I doubt it would be in the course of the of spectators; but the good citizens of Blois could not pardon Monsieur for having chosen their gay city for an abode in which to indulge melancholy and oppress him, and as often as they caught a glimpse of his manipulation, they stole away gaping, or drew back their heads into the interior deals of citizens, to escape the soporific influence of that long-dressed hoodlum with purplish wrinkles and flaming cheekbones, and blue eyes, and that languid address; so that the worthy prince was almost certain to find the streets deserted whenever he chanced to glance at them.\"]\n",
        "\n",
        "label = ['orig', 'Nabokov', 'Austen', \n",
        "         'orig', 'Nabokov',\n",
        "         'orig', 'Nabokov']\n",
        "\n",
        "df = pd.DataFrame({'text': txt, \n",
        "                   'label': label})\n",
        "# df.to_csv('./gdrive/My Drive/DL/Style/final_examples.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlHdkeqbzfRG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "c7a83e45-f533-4ce4-e8f6-5a6f2ccf02f3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib\n",
        "import tensorflow as tf\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "import nltk\n",
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "\n",
        "!wget 'https://nlp.stanford.edu/software/stanford-ner-2018-10-16.zip'\n",
        "!unzip stanford-ner-2018-10-16.zip\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "st = StanfordNERTagger('/content/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
        "                       '/content/stanford-ner-2018-10-16/stanford-ner.jar',\n",
        "                       encoding='utf-8')\n",
        "\n",
        "# def mask_names(text, filter):\n",
        "#   classified_text = word_tokenize(text) # text.split()\n",
        "#   l = ''\n",
        "#   for i in classified_text:\n",
        "#     if str(i[0]).lower() in filter:\n",
        "#       l += ' ' + '<MASK>'\n",
        "#     elif i in string.punctuation: \n",
        "#       l += i\n",
        "#     else:\n",
        "#       l += ' ' + i\n",
        "#   return l\n",
        "\n",
        "names_Dumas = ['buckingham', 'comte', 'boxtel', 'montalais', 'van baerle', \n",
        "               'baerle', 'malicorne', 'louis', 'chicot', 'mazarin', \n",
        "               'fouquet', 'guiche', 'rosa', 'd artagnan', 'artagnan', \n",
        "               'raoul', 'athos', 'aramis', 'cornelius', 'porthos']\n",
        "\n",
        "names_Nabokov = ['dreyer', 'fyodor', 'martha', 'chernyshevski', 'sebastian',\n",
        "                 'franz', 'lolita', 'albinus', 'cincinnatus', 'van', \n",
        "                 'ada', 'martin', 'krug', 'luzhin']\n",
        "\n",
        "names_Austen = ['lucy', 'wentworth', 'knightley', 'morland', 'wickham', \n",
        "                'tilney', 'mansfield', 'isabella', 'henry', 'william', \n",
        "                'elliot', 'darcy', 'willoughby', 'edward', 'crawford', \n",
        "                'edmund', 'anne', 'jane', 'harriet', 'marianne', \n",
        "                'elizabeth', 'emma', 'elinor', 'catherine', 'fanny']\n",
        "\n",
        "NAMES = names_Dumas + names_Nabokov + names_Austen\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "def mask_names(text, filter):\n",
        "  classified_text = st.tag(word_tokenize(text)) # text.split()\n",
        "  l = ''\n",
        "  for i in classified_text:\n",
        "    if i[1] == 'PERSON' or str(i[0]).lower() in filter:\n",
        "      l += ' ' + '<MASK>'\n",
        "    elif str(i[0]) in string.punctuation: \n",
        "      l += i[0]\n",
        "    else:\n",
        "      l += ' ' + i[0]\n",
        "  return l"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "--2019-12-08 11:35:23--  https://nlp.stanford.edu/software/stanford-ner-2018-10-16.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180358328 (172M) [application/zip]\n",
            "Saving to: ‘stanford-ner-2018-10-16.zip.4’\n",
            "\n",
            "stanford-ner-2018-1 100%[===================>] 172.00M  53.4MB/s    in 3.5s    \n",
            "\n",
            "2019-12-08 11:35:27 (48.7 MB/s) - ‘stanford-ner-2018-10-16.zip.4’ saved [180358328/180358328]\n",
            "\n",
            "Archive:  stanford-ner-2018-10-16.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of stanford-ner-2018-10-16.zip or\n",
            "        stanford-ner-2018-10-16.zip.zip, and cannot find stanford-ner-2018-10-16.zip.ZIP, period.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khKpnTzNQ61i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "bbadba07-9aec-4509-bd7e-8892732656b2"
      },
      "source": [
        "l = []\n",
        "for i in txt:\n",
        "  l.append(mask_names(i, NAMES))\n",
        "  print(len(word_tokenize(i)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "381\n",
            "331\n",
            "418\n",
            "194\n",
            "223\n",
            "136\n",
            "144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpghFrylSfpi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2c3b3261-7780-4c81-d609-1de6890d2eac"
      },
      "source": [
        "model = Embeddings()\n",
        "\n",
        "l_text = []\n",
        "for sentence in tqdm(l):\n",
        "    l_text.append(model.sentence2vec(sentence, layers=model.LAST_LAYER))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 2759976.49B/s]\n",
            "100%|██████████| 313/313 [00:00<00:00, 108992.71B/s]\n",
            "100%|██████████| 440473133/440473133 [00:06<00:00, 69003983.59B/s]\n",
            "100%|██████████| 7/7 [00:06<00:00,  1.29it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovecTYc9SgdC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "44eff3b6-a971-4227-fb85-8cde4bf7c9b3"
      },
      "source": [
        "model = load_model('./gdrive/My Drive/DL/Style/DistilBERT.h5')\n",
        "pred = model.predict(pd.DataFrame(l_text))\n",
        "pred"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01131631, 0.0077914 , 0.98089224],\n",
              "       [0.00872069, 0.00714286, 0.98413646],\n",
              "       [0.0506387 , 0.01011489, 0.9392464 ],\n",
              "       [0.98696554, 0.00733038, 0.0057041 ],\n",
              "       [0.9790092 , 0.01385594, 0.00713488],\n",
              "       [0.00963072, 0.9838552 , 0.00651407],\n",
              "       [0.00877737, 0.9843924 , 0.00683032]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4xFPuyqUWmm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f4571eb-1855-473e-99d5-8b056e2a5306"
      },
      "source": [
        "0.00683032 - 0.00651407"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0003162499999999997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-mIOP0RVYqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}